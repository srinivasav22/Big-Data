	
Prerequisite for the activity:
==============================	
	Step 1: Opening mysql and defining the tables.
	mysql -h <ipaddress> -u insofeadmin -p < /home/2463B52/BIonHadoopNew/table_definitions.sql
 
	Step 2: Load data into the database.
	mysql -h <ipaddress> -u insofeadmin -p < /home/2463B52/BIonHadoopNew/bulkload.sql

BI on Hadoop Activity :
=======================
Step 1: Import the data into HDFS using Sqoop import.
sh /home/2463B52/BIonHadoopNew/sqoopimportjobdefinitions.sh initialimport 172.16.0.218 insofe_empdb /user/2463B52/employeesDBtho
 
Step 2: To check the number of records loaded into HDFS.  
sh /home/2463B52/BIonHadoopNew/hdfsRowCount.sh

	Prerequisit for the remaining steps:
	=====================================
	Import tables in mysql with the delta loads.
	mysql -u insofeadmin -p < /home/2463B52/BIonHadoopNew/incremental_loading.sql

Step 3: Delta load moving to HDFS via sqoop incremetal import
sh /home/2463B52/BIonHadoopNew/sqoopincimports.sh  172.16.0.218 insofe_empdb 

Step 4:  Verify the delta records are brought to hdfs
sh /home/2463B52/BIonHadoopNew/hdfsRowCount.sh

Step 5:  Hive table creations
hive -f /home/2463B52/BIonHadoopNew/hivetabledefs.hql

Step 6: Aggregated tables create using Pig
pig -x mapreduce -useHCatalog /home/2463B52/BIonHadoopNew/TransformationsScript.pig

Steps 7: Verify the results tables in Hdfs
hdfs dfs -ls /user/2463B52/results/active_employees_data/

Step 8: Hive aggregation for results tables
hive -f /home/2463B52/BIonHadoopNew/hiveaggregatedtables.hql

Step 9: Mysql results table creation.
mysql -u insofeadmin -p < /home/2463B52/BIonHadoopNew/resultsinsql.sql

Step 10: Tables exported to mysql
sh /home/2463B52/BIonHadoopNew/sqoopexports.sh 172.16.0.218

Step 11: Visualize the data 


PART - B Spark ML

PART - A Descriptive Analysis (hive)




 

