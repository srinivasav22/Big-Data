{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Operations using Spark DataFrames and Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this activity we will understand\n",
    "-  What are DataFrames in Spark ?\n",
    "-  Different ways to create a DataFrames\n",
    "-  What are Spark Transformations & Actions\n",
    "-  Verify Summary Statistics\n",
    "-  Spark SQL\n",
    "-  Performance Comparison of Spark DataFrame and Spark SQL\n",
    "-  Column References\n",
    "-  Converting to Spark Types - Literals\n",
    "-  Add/Rename/Remove Columns\n",
    "-  TypeCasting\n",
    "-  Column differences\n",
    "-  Pair-wise frequencies\n",
    "-  Remove duplicates\n",
    "-  Working with Nulls\n",
    "-  Filtering the rows\n",
    "-  Aggregations\n",
    "-  Joins\n",
    "-  Random Samples\n",
    "-  Random Splits\n",
    "-  Map Transformations\n",
    "-  Sorting\n",
    "-  Union\n",
    "-  String Manipulations\n",
    "-  Regular Expressions\n",
    "-  Working with Dates and Time Stamp\n",
    "-  User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Python - Spark environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create spark context and session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Python Spark DataFrames and Spark SQL\")\\\n",
    "        .master('local[*]')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://x.insofe.edu.in:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark DataFrames and Spark SQL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x3cd3790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Spark DataFrame **\n",
    "\n",
    "#### A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. \n",
    "<br> The list that defines the columns and the types within those columns is called the schema. \n",
    "<br> One can think of a DataFrame as a spreadsheet with named columns.\n",
    "<br> A spreadsheet sits on one computer in one specific location, whereas a Spark DataFrame can span thousands of computers.\n",
    "<br> The reason for putting the data on more than one computer should be intuitive: \n",
    "<br>     either the data is too large to fit on one machine or \n",
    "<br>     it would simply take too long to perform that computation on one machine.\n",
    "\n",
    "#### NOTE\n",
    "Spark has several core abstractions: Datasets, DataFrames, SQL Tables, and Resilient Distributed Datasets (RDDs). \n",
    "<br> These different abstractions all represent distributed collections of data. \n",
    "<br> The easiest and most efficient are DataFrames, which are available in all languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Representation\n",
    "- **Pandas** - DataFrames represented on a single machine as Python data structures\n",
    "- **RDDs** - Spark’s foundational structure Resilient Distributed Dataset is represented as a reference to partitioned data without types\n",
    "- **DataFrames** - Spark’s strongly typed optimized distributed collection of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe with one column containing 100 rows with values from 0 to 99.\n",
    "This range of numbers represents a distributed collection. \n",
    "<br> When run on a cluster, each part of this range of numbers exists on a different executor. \n",
    "<br> This is a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRangeDF = spark.range(100).toDF('Number')\n",
    "myRangeDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Number: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRangeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "myDF = spark.createDataFrame([[1, 'Alpha', 12, 'Bhimber'],\n",
    "                              [2, 'Beta', 15, 'Hotspring'],\n",
    "                              [3, 'Charlie', 10, 'Kel'],\n",
    "                              [4, 'Delta', 15, 'Lipa']], ['ID', 'T_Name', 'Strength', 'Place'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+---------+\n",
      "| ID| T_Name|Strength|    Place|\n",
      "+---+-------+--------+---------+\n",
      "|  1|  Alpha|      12|  Bhimber|\n",
      "|  2|   Beta|      15|Hotspring|\n",
      "|  3|Charlie|      10|      Kel|\n",
      "|  4|  Delta|      15|     Lipa|\n",
      "+---+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Transformations & Actions\n",
    "\n",
    "### Transformations\n",
    "In Spark, the core data structures are immutable, meaning they cannot be changed after they’re created.\n",
    "<br> To “change” a DataFrame, you need to instruct Spark how you would like to modify it to do what you want.\n",
    "<br> These instructions are called transformations.\n",
    "<br> Transformations are the core of how you express your business logic using Spark.\n",
    "<br> Transformations are simply ways of specifying different series of data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Number: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evenDF = myRangeDF.where(\"number % 2 = 0\")\n",
    "evenDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these return no output. <br>This is because we specified only an abstract transformation, and Spark will not act on transformations until we call an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "Transformations allow us to build up our logical transformation plan. \n",
    "<br> To trigger the computation, we run an action.\n",
    "<br> An action instructs Spark to compute a result from a series of transformations. \n",
    "<br> The simplest action is count, which gives us the total number of records in the DataFrame:\n",
    "\n",
    "#### There are 3 types of actions\n",
    "Actions to view data in the console\n",
    "<br>Actions to collect data to native objects in the respective language\n",
    "<br>Actions to write to output data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evenDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "|    10|\n",
      "|    12|\n",
      "|    14|\n",
      "|    16|\n",
      "|    18|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evenDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interoperating with RDDs\n",
    "\n",
    "<br> Spark SQL supports two different methods for converting existing RDDs into DataFrames. \n",
    "* The first method uses reflection to infer the schema of an RDD that contains specific types of objects. \n",
    "  <br> This reflection based approach leads to more concise code and works well when you already know the schema while writing your Spark application.\n",
    "\n",
    "* The second method for creating DataFrames is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. \n",
    "  <br> While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfsroot/data/home/pavanw/B52/SparkSQL_DF\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 27156\r\n",
      "-rw-rw-r-- 1 pavanw pavanw    14357 Feb 14 15:33 SparkSQL_datasets.ipynb\r\n",
      "-rw-r--r-- 1 pavanw pavanw   265945 Feb 14 15:33 TCS_NSE.csv\r\n",
      "-rw-r--r-- 1 pavanw pavanw   140043 Feb 14 15:33 temp_data.txt\r\n",
      "-rw-r--r-- 1 pavanw pavanw   937409 Feb 14 15:33 test_sample10.csv\r\n",
      "-rw-r--r-- 1 pavanw pavanw 26062111 Feb 14 15:33 train_sample10.csv\r\n",
      "drwxrwxr-x 2 pavanw pavanw       10 Feb 14 17:18 spark-warehouse\r\n",
      "-rw-rw-r-- 1 pavanw pavanw     9789 Feb 14 18:17 Diff_between_global&TempView.ipynb\r\n",
      "-rw-rw-r-- 1 pavanw pavanw   181610 Feb 15 17:55 Spark_SQL&DF.ipynb\r\n",
      "-rw-rw-r-- 1 pavanw pavanw   181610 Feb 15 17:56 Spark_SQL&DF-Copy1.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/pavanw/SparkSQL_DF': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/pavanw/SparkSQL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/pavanw/SparkSQL_DF/temp_data.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put /home/pavanw/B52/SparkSQL_DF/temp_data.txt /user/pavanw/SparkSQL_DF/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwxr-xr-x   - pavanw pavanw          0 2019-02-15 17:52 /user/pavanw/SparkSQL_DF/TCS_JSON\r\n",
      "-rw-r--r--   3 pavanw pavanw     265945 2019-02-15 17:50 /user/pavanw/SparkSQL_DF/TCS_NSE.csv\r\n",
      "-rw-r--r--   3 pavanw pavanw     140043 2019-02-14 15:06 /user/pavanw/SparkSQL_DF/temp_data.txt\r\n",
      "-rw-r--r--   3 pavanw pavanw     937409 2019-02-14 15:39 /user/pavanw/SparkSQL_DF/test_sample10.csv\r\n",
      "-rw-r--r--   3 pavanw pavanw   26062111 2019-02-14 16:50 /user/pavanw/SparkSQL_DF/train_sample10.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/pavanw/SparkSQL_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the Schema Using Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from Source\n",
    "tempRDD_HDFS = spark.sparkContext.textFile(\"/user/pavanw/SparkSQL_DF/temp_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1901\\t-78\\t1', u'1901\\t-72\\t1', u'1901\\t-94\\t1', u'1901\\t-61\\t1']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempRDD_HDFS.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'1901', u'-78', u'1'],\n",
       " [u'1901', u'-72', u'1'],\n",
       " [u'1901', u'-94', u'1'],\n",
       " [u'1901', u'-61', u'1']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "splitRDD = tempRDD_HDFS.map(lambda line: line.split(\"\\t\"))\n",
    "splitRDD.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(random=-78, status=1, year=u'1901'),\n",
       " Row(random=-72, status=1, year=u'1901'),\n",
       " Row(random=-94, status=1, year=u'1901')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemafiedRDD = splitRDD.map(lambda line: Row(year=line[0], random=int(line[1]), status=int(line[2])))\n",
    "schemafiedRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+\n",
      "|random|status|year|\n",
      "+------+------+----+\n",
      "|   -78|     1|1901|\n",
      "|   -72|     1|1901|\n",
      "|   -94|     1|1901|\n",
      "|   -61|     1|1901|\n",
      "+------+------+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Infer schema and register dataframe as table\n",
    "tempDF = spark.createDataFrame(schemafiedRDD)\n",
    "tempDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programmatically Specifying the Schema\n",
    "- Create an RDD of tuples or lists from the original RDD;\n",
    "- Create the schema represented by a StructType matching the structure of tuples or lists in the RDD created in the step 1.\n",
    "- Apply the schema to the RDD via createDataFrame method provided by SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfsroot/data/home/pavanw/B52/SparkSQL_DF\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 27156\r\n",
      "-rw-rw-r-- 1 pavanw pavanw    14357 Feb 14 15:33 SparkSQL_datasets.ipynb\r\n",
      "-rw-r--r-- 1 pavanw pavanw   265945 Feb 14 15:33 TCS_NSE.csv\r\n",
      "-rw-r--r-- 1 pavanw pavanw   140043 Feb 14 15:33 temp_data.txt\r\n",
      "-rw-r--r-- 1 pavanw pavanw   937409 Feb 14 15:33 test_sample10.csv\r\n",
      "-rw-r--r-- 1 pavanw pavanw 26062111 Feb 14 15:33 train_sample10.csv\r\n",
      "drwxrwxr-x 2 pavanw pavanw       10 Feb 14 17:18 spark-warehouse\r\n",
      "-rw-rw-r-- 1 pavanw pavanw     9789 Feb 14 18:17 Diff_between_global&TempView.ipynb\r\n",
      "-rw-rw-r-- 1 pavanw pavanw   181610 Feb 15 17:55 Spark_SQL&DF.ipynb\r\n",
      "-rw-rw-r-- 1 pavanw pavanw   181610 Feb 15 17:56 Spark_SQL&DF-Copy1.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/pavanw/SparkSQL_DF/test_sample10.csv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put /home/pavanw/B52/SparkSQL_DF/test_sample10.csv /user/pavanw/SparkSQL_DF/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwxr-xr-x   - pavanw pavanw          0 2019-02-15 17:52 /user/pavanw/SparkSQL_DF/TCS_JSON\r\n",
      "-rw-r--r--   3 pavanw pavanw     265945 2019-02-15 17:50 /user/pavanw/SparkSQL_DF/TCS_NSE.csv\r\n",
      "-rw-r--r--   3 pavanw pavanw     140043 2019-02-14 15:06 /user/pavanw/SparkSQL_DF/temp_data.txt\r\n",
      "-rw-r--r--   3 pavanw pavanw     937409 2019-02-14 15:39 /user/pavanw/SparkSQL_DF/test_sample10.csv\r\n",
      "-rw-r--r--   3 pavanw pavanw   26062111 2019-02-14 16:50 /user/pavanw/SparkSQL_DF/train_sample10.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/pavanw/SparkSQL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Records with header: ', 23380)\n",
      "\n",
      "First Two Records Before Removing Header\n",
      "\n",
      "[u'User_ID,Product_ID,Gender,Age,Occupation,City_Category,Stay_In_Current_City_Years,Marital_Status,Product_Category_1,Product_Category_2,Product_Category_3', u'1000029,P00111542,M,36-45,7,C,1,0,2,17,']\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "testRDD = spark.sparkContext.textFile(\"/user/pavanw/SparkSQL_DF/test_sample10.csv\")\n",
    "print(\"Total Records with header: \", ?)\n",
    "print(\"\\nFirst Two Records Before Removing Header\\n\")\n",
    "print(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Records without header: ', 23379)\n",
      "\n",
      "First Two Records After Removing Header\n",
      "\n",
      "[u'1000029,P00111542,M,36-45,7,C,1,0,2,17,', u'1000034,P00265242,F,18-25,0,A,0,0,5,8,']\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "header = ?\n",
    "testRDD = ?\n",
    "print(\"Total Records without header: \", ?)\n",
    "print(\"\\nFirst Two Records After Removing Header\\n\")\n",
    "print(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Two Records After Split/Parsing\n",
      "\n",
      "[[u'1000029', u'P00111542', u'M', u'36-45', u'7', u'C', u'1', u'0', u'2', u'17', u''], [u'1000034', u'P00265242', u'F', u'18-25', u'0', u'A', u'0', u'0', u'5', u'8', u'']]\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "# Split the data into individual columns\n",
    "splitRDD = ?\n",
    "print(\"\\nFirst Two Records After Split/Parsing\\n\")\n",
    "print(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe for the above Data\n",
    "1. Define Schema\n",
    "2. Create dataframe using the above schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame using the above schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=u'1000029', Product_ID=u'P00111542', Gender=u'M', Age=u'36-45', Occupation=u'7', City_Category=u'C', Stay_In_Current_City_Years=u'1', Marital_Status=u'0', Product_Category_1=u'2', Product_Category_2=u'17', Product_Category_3=u''),\n",
       " Row(User_ID=u'1000034', Product_ID=u'P00265242', Gender=u'F', Age=u'18-25', Occupation=u'0', City_Category=u'A', Stay_In_Current_City_Years=u'0', Marital_Status=u'0', Product_Category_1=u'5', Product_Category_2=u'8', Product_Category_3=u''),\n",
       " Row(User_ID=u'1000053', Product_ID=u'P0097342', Gender=u'M', Age=u'26-35', Occupation=u'0', City_Category=u'B', Stay_In_Current_City_Years=u'1', Marital_Status=u'0', Product_Category_1=u'1', Product_Category_2=u'15', Product_Category_3=u'16'),\n",
       " Row(User_ID=u'1000080', Product_ID=u'P00112142', Gender=u'M', Age=u'55+', Occupation=u'1', City_Category=u'C', Stay_In_Current_City_Years=u'3', Marital_Status=u'1', Product_Category_1=u'1', Product_Category_2=u'2', Product_Category_3=u'14'),\n",
       " Row(User_ID=u'1000081', Product_ID=u'P00109142', Gender=u'F', Age=u'26-35', Occupation=u'0', City_Category=u'A', Stay_In_Current_City_Years=u'1', Marital_Status=u'1', Product_Category_1=u'8', Product_Category_2=u'17', Product_Category_3=u'')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## \n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
      "|User_ID|Product_ID|Gender|Age  |Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
      "|1000029|P00111542 |M     |36-45|7         |C            |1                         |0             |2                 |17                |                  |\n",
      "|1000034|P00265242 |F     |18-25|0         |A            |0                         |0             |5                 |8                 |                  |\n",
      "|1000053|P0097342  |M     |26-35|0         |B            |1                         |0             |1                 |15                |16                |\n",
      "|1000080|P00112142 |M     |55+  |1         |C            |3                         |1             |1                 |2                 |14                |\n",
      "|1000081|P00109142 |F     |26-35|0         |A            |1                         |1             |8                 |17                |                  |\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: string (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      " |-- Product_Category_1: string (nullable = true)\n",
      " |-- Product_Category_2: string (nullable = true)\n",
      " |-- Product_Category_3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Verify/Print Schema\n",
    "testDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23379"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataframe directly by reading a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfsroot/data/home/pavanw/B52/SparkSQL_DF\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 27156\r\n",
      "-rw-rw-r-- 1 pavanw pavanw    14357 Feb 14 15:33 SparkSQL_datasets.ipynb\r\n",
      "-rw-r--r-- 1 pavanw pavanw   265945 Feb 14 15:33 TCS_NSE.csv\r\n",
      "-rw-r--r-- 1 pavanw pavanw   140043 Feb 14 15:33 temp_data.txt\r\n",
      "-rw-r--r-- 1 pavanw pavanw   937409 Feb 14 15:33 test_sample10.csv\r\n",
      "-rw-r--r-- 1 pavanw pavanw 26062111 Feb 14 15:33 train_sample10.csv\r\n",
      "drwxrwxr-x 2 pavanw pavanw       10 Feb 14 17:18 spark-warehouse\r\n",
      "-rw-rw-r-- 1 pavanw pavanw     9789 Feb 14 18:17 Diff_between_global&TempView.ipynb\r\n",
      "-rw-rw-r-- 1 pavanw pavanw   181610 Feb 15 17:55 Spark_SQL&DF.ipynb\r\n",
      "-rw-rw-r-- 1 pavanw pavanw   181610 Feb 15 17:56 Spark_SQL&DF-Copy1.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/pavanw/SparkSQL_DF/train_sample10.csv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put /home/pavanw/B52/SparkSQL_DF/train_sample10.csv /user/pavanw/SparkSQL_DF/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwxr-xr-x   - pavanw pavanw          0 2019-02-15 17:52 /user/pavanw/SparkSQL_DF/TCS_JSON\r\n",
      "-rw-r--r--   3 pavanw pavanw     265945 2019-02-15 17:50 /user/pavanw/SparkSQL_DF/TCS_NSE.csv\r\n",
      "-rw-r--r--   3 pavanw pavanw     140043 2019-02-14 15:06 /user/pavanw/SparkSQL_DF/temp_data.txt\r\n",
      "-rw-r--r--   3 pavanw pavanw     937409 2019-02-14 15:39 /user/pavanw/SparkSQL_DF/test_sample10.csv\r\n",
      "-rw-r--r--   3 pavanw pavanw   26062111 2019-02-14 16:50 /user/pavanw/SparkSQL_DF/train_sample10.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/pavanw/SparkSQL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|\n",
      "|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|              null|              null|    1422|\n",
      "|1000001| P00085442|     F|0-17|        10|            A|                         2|             0|                12|              14.0|              null|    1057|\n",
      "|1000002| P00285442|     M| 55+|        16|            C|                        4+|             0|                 8|              null|              null|    7969|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=1000001, Product_ID=u'P00069042', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n",
       " Row(User_ID=1000001, Product_ID=u'P00248942', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6.0, Product_Category_3=14.0, Purchase=15200),\n",
       " Row(User_ID=1000001, Product_ID=u'P00087842', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=12, Product_Category_2=None, Product_Category_3=None, Purchase=1422),\n",
       " Row(User_ID=1000001, Product_ID=u'P00085442', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=12, Product_Category_2=14.0, Product_Category_3=None, Purchase=1057),\n",
       " Row(User_ID=1000002, Product_ID=u'P00285442', Gender=u'M', Age=u'55+', Occupation=16, City_Category=u'C', Stay_In_Current_City_Years=u'4+', Marital_Status=0, Product_Category_1=8, Product_Category_2=None, Product_Category_3=None, Purchase=7969)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: double (nullable = true)\n",
      " |-- Product_Category_3: double (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Verify/Print Schema\n",
    "trainDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=1000001, Product_ID=u'P00069042', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n",
       " Row(User_ID=1000001, Product_ID=u'P00248942', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6.0, Product_Category_3=14.0, Purchase=15200)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To Show first n observations\n",
    "## Use head operation to see first n observations (say, 2 observations). \n",
    "## Head operation in PySpark is similar to head operation in Pandas.\n",
    "trainDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>City_Category</th>\n",
       "      <th>Stay_In_Current_City_Years</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Product_Category_1</th>\n",
       "      <th>Product_Category_2</th>\n",
       "      <th>Product_Category_3</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>550066</th>\n",
       "      <td>1006038</td>\n",
       "      <td>P00375436</td>\n",
       "      <td>F</td>\n",
       "      <td>55+</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550067</th>\n",
       "      <td>1006039</td>\n",
       "      <td>P00371644</td>\n",
       "      <td>F</td>\n",
       "      <td>46-50</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>4+</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        User_ID Product_ID Gender    Age  Occupation City_Category  \\\n",
       "550066  1006038  P00375436      F    55+           1             C   \n",
       "550067  1006039  P00371644      F  46-50           0             B   \n",
       "\n",
       "       Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
       "550066                          2               0                  20   \n",
       "550067                         4+               1                  20   \n",
       "\n",
       "        Product_Category_2  Product_Category_3  Purchase  \n",
       "550066                 NaN                 NaN       365  \n",
       "550067                 NaN                 NaN       490  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.toPandas().tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records count in train dataset is 550068\n",
      "Total records count in test dataset is 23379\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "## To Count the number of rows in DataFrame\n",
    "print('Total records count in train dataset is {}'.format(trainDF.?))\n",
    "print('Total records count in test dataset is {}'.format(testDF.?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns count in train dataset is 12\n",
      "\n",
      "\n",
      "Columns in train dataset are: ['User_ID', 'Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Purchase'] \n",
      "\n",
      "Total Columns count in test dataset is 11\n",
      "\n",
      "\n",
      "Columns in test dataset are: ['User_ID', 'Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Columns count and column names\n",
    "print(\"Total Columns count in train dataset is {}\".format(?))\n",
    "print(\"\\n\\nColumns in train dataset are: {} \\n\".format(?))\n",
    "\n",
    "print(\"Total Columns count in test dataset is {}\".format(?))\n",
    "print(\"\\n\\nColumns in test dataset are: {} \\n\".format(?))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|           User_ID|Product_ID|Gender|   Age|       Occupation|City_Category|Stay_In_Current_City_Years|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|         Purchase|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            550068|    550068|550068|550068|           550068|       550068|                    550068|             550068|            550068|            376430|            166821|           550068|\n",
      "|   mean|1003028.8424013031|      null|  null|  null|8.076706879876669|         null|         1.468494139793958|0.40965298835780306| 5.404270017525106| 9.842329251122386|12.668243206790512|9263.968712959126|\n",
      "| stddev|1727.5915855308244|      null|  null|  null|6.522660487341818|         null|        0.9890866807573161| 0.4917701263173307|3.9362113692013874| 5.086589648693473| 4.125337631575284|5023.065393820589|\n",
      "|    min|           1000001| P00000142|     F|  0-17|                0|            A|                         0|                  0|                 1|               2.0|               3.0|               12|\n",
      "|    max|           1006040|  P0099942|     M|   55+|               20|            C|                        4+|                  1|                20|              18.0|              18.0|            23961|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To get the summary statistics (mean, standard deviance, min ,max , count) of numerical columns in a DataFrame\n",
    "trainDF.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|         Purchase|\n",
      "+-------+-----------------+\n",
      "|  count|           550068|\n",
      "|   mean|9263.968712959126|\n",
      "| stddev|5023.065393820589|\n",
      "|    min|               12|\n",
      "|    max|            23961|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check what happens when we specify the name of a categorical / String columns in describe operation.\n",
    "## describe operation is working for String type column but the output for mean, stddev are null and \n",
    "## min & max values are calculated based on ASCII value of categories.\n",
    "trainDF.describe('Purchase').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "With Spark SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure SQL. \n",
    "<br>There is no performance difference between writing SQL queries or writing DataFrame code, \n",
    "<br>they both “compile” to the same underlying plan that we specify in DataFrame code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Create view/table\n",
    "trainDF.createOrReplaceTempView(\"trainDFTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Verify Dataframe\n",
    "trainDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=1000001, Product_ID=u'P00069042', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n",
       " Row(User_ID=1000001, Product_ID=u'P00248942', Gender=u'F', Age=u'0-17', Occupation=10, City_Category=u'A', Stay_In_Current_City_Years=u'2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6.0, Product_Category_3=14.0, Purchase=15200)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Verify Dataframe\n",
    "trainDF.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Verify Table\n",
    "spark.sql(\"SELECT * FROM trainDFTable LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Comparison Spark DataFrame vs Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Age#147], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(Age#147, 200)\n",
      "   +- *(1) HashAggregate(keys=[Age#147], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [Age#147] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://bigdata/user/pavanw/SparkSQL_DF/train_sample10.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Age:string>\n"
     ]
    }
   ],
   "source": [
    "#dataframeWay = trainDF.where(trainDF.Purchase>15000).count()\n",
    "dataframeWay = trainDF.groupBy('Age').count()\n",
    "dataframeWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[Age#147], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(Age#147, 200)\n",
      "   +- *(1) HashAggregate(keys=[Age#147], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [Age#147] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://bigdata/user/pavanw/SparkSQL_DF/train_sample10.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Age:string>\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"SELECT Age, count(*) FROM trainDFTable GROUP BY Age\")\n",
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select & SelectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple ways of referring a column in a dataframe\n",
    "from pyspark.sql.functions import expr, col, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|User_ID|\n",
      "+-------+\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000002|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(col(\"User_ID\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|User_ID|\n",
      "+-------+\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000002|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(column(\"User_ID\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|User_ID|\n",
      "+-------+\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000002|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(\"User_ID\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| userID|\n",
      "+-------+\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000002|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(expr(\"User_ID AS userID\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "| userID|User_ID|User_ID|User_ID|\n",
      "+-------+-------+-------+-------+\n",
      "|1000001|1000001|1000001|1000001|\n",
      "|1000001|1000001|1000001|1000001|\n",
      "|1000001|1000001|1000001|1000001|\n",
      "|1000001|1000001|1000001|1000001|\n",
      "|1000002|1000002|1000002|1000002|\n",
      "+-------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(expr(\"User_ID AS userID\") ,\n",
    "               col(\"User_ID\"), \n",
    "               column(\"User_ID\"), \"User_ID\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| userID|\n",
      "+-------+\n",
      "|1000001|\n",
      "|1000001|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT User_ID AS userID FROM trainDFTable\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "| userID|productID|\n",
      "+-------+---------+\n",
      "|1000001|P00069042|\n",
      "|1000001|P00248942|\n",
      "+-------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.selectExpr(\"User_ID AS userID\", \"Product_ID AS productID\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+\n",
      "|User_ID|Product_ID| Age|\n",
      "+-------+----------+----+\n",
      "|1000001| P00069042|0-17|\n",
      "|1000001| P00248942|0-17|\n",
      "+-------+----------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(\"User_ID\", \"Product_ID\", \"Age\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to Spark Types (Literals)\n",
    "Sometimes we need to pass explicit values into Spark that aren’t a new column but are just a value in all the rows. This might be a constant value or something we’ll need to compare to later on. The way we do this is through literals. \n",
    "This is basically a translation from a given programming language’s literal value to one that Spark understands. \n",
    "Literals are expressions and can be used in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "trainDF.select(\"*\", lit(1).alias('One')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## In SQL, literals are just the specific value.\n",
    "spark.sql(\"SELECT *, 1 as One FROM trainDFTable LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## More Formal way\n",
    "trainDF.withColumn(\"One\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|One|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|  1|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|  1|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT *, 1 AS One FROM trainDFTable LIMIT 2\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|SameCategoryCode|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|            null|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|           false|\n",
      "|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|              null|              null|    1422|            null|\n",
      "|1000001| P00085442|     F|0-17|        10|            A|                         2|             0|                12|              14.0|              null|    1057|           false|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF = trainDF.withColumn(\"SameCategoryCode\", trainDF[\"Product_Category_1\"] == trainDF[\"Product_Category_2\"])\n",
    "tempDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---------------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|SimilarCategory|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---------------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|           null|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|          false|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "tempDF.withColumnRenamed(\"SameCategoryCode\", \"SimilarCategory\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "tempDF.drop(\"SameCategoryCode\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing a Column’s Type (cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: double (nullable = true)\n",
      " |-- Product_Category_3: double (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      " |-- SameCategoryCode: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: double (nullable = true)\n",
      " |-- Product_Category_3: double (nullable = true)\n",
      " |-- Purchase: string (nullable = true)\n",
      " |-- SameCategoryCode: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.withColumn(\"Purchase\", col(\"Purchase\").cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in Product_ID's in train dataset are 3631\n",
      "Distinct values in Product_ID's in test dataset are 2762\n"
     ]
    }
   ],
   "source": [
    "## To find the number of distinct product in train and test datasets\n",
    "## To calculate the number of distinct products in train and test datasets apply distinct operation.\n",
    "train_product_distinct_count = trainDF.select(\"Product_ID\").distinct().count() \n",
    "print(\"Distinct values in Product_ID's in train dataset are {}\".format(train_product_distinct_count))\n",
    "\n",
    "##\n",
    "test_product_distinct_count = \n",
    "print(\"Distinct values in Product_ID's in test dataset are {}\".format(test_product_distinct_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences in two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Product_IDs in test but not in train are 5\n",
      "Count of Product_IDs in train but not in test are 874\n"
     ]
    }
   ],
   "source": [
    "## From the above we can see the train file has more categories than test file. \n",
    "## Let us check what are the categories for Product_ID, which are in test file but not in train file by \n",
    "## applying subtract operation.\n",
    "## We can do the same for all categorical features.\n",
    "diff_cat_in_test_train = testDF.select('Product_ID').subtract(trainDF.select('Product_ID'))\n",
    "print(\"Count of Product_IDs in test but not in train are {}\".format(diff_cat_in_test_train.count()))\n",
    "\n",
    "diff_cat_in_train_test = \n",
    "print(\"Count of Product_IDs in train but not in test are {}\".format(diff_cat_in_train_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair wise Frequencies - Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|Age_Gender|    F|     M|\n",
      "+----------+-----+------+\n",
      "|      0-17| 5083| 10019|\n",
      "|     46-50|13199| 32502|\n",
      "|     18-25|24628| 75032|\n",
      "|     36-45|27170| 82843|\n",
      "|       55+| 5083| 16421|\n",
      "|     51-55| 9894| 28607|\n",
      "|     26-35|50752|168835|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To calculate pair wise frequency of categorical columns\n",
    "## Use crosstab operation on DataFrame to calculate the pair wise frequency of columns. \n",
    "## Apply crosstab operation on ‘Age’ and ‘Gender’ columns of train DataFrame.\n",
    "trainDF.crosstab('Age', 'Gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|  Age|Gender| count|\n",
      "+-----+------+------+\n",
      "|51-55|     F|  9894|\n",
      "|18-25|     M| 75032|\n",
      "| 0-17|     F|  5083|\n",
      "|46-50|     M| 32502|\n",
      "|18-25|     F| 24628|\n",
      "|  55+|     M| 16421|\n",
      "|  55+|     F|  5083|\n",
      "|36-45|     M| 82843|\n",
      "|26-35|     F| 50752|\n",
      "| 0-17|     M| 10019|\n",
      "|36-45|     F| 27170|\n",
      "|51-55|     M| 28607|\n",
      "|26-35|     M|168835|\n",
      "|46-50|     F| 13199|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupBy('Age', 'Gender').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "|  Age|    F|     M|\n",
      "+-----+-----+------+\n",
      "|18-25|24628| 75032|\n",
      "|26-35|50752|168835|\n",
      "| 0-17| 5083| 10019|\n",
      "|46-50|13199| 32502|\n",
      "|51-55| 9894| 28607|\n",
      "|36-45|27170| 82843|\n",
      "|  55+| 5083| 16421|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select Age,\n",
    "    sum(case when Gender = 'F' then 1 else 0 end) as F,\n",
    "    sum(case when Gender = 'M' then 1 else 0 end) as M\n",
    "from trainDFTable\n",
    "group by Age\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|  Age|Gender|\n",
      "+-----+------+\n",
      "|51-55|     F|\n",
      "|18-25|     M|\n",
      "| 0-17|     F|\n",
      "|46-50|     M|\n",
      "|18-25|     F|\n",
      "|  55+|     M|\n",
      "|  55+|     F|\n",
      "|36-45|     M|\n",
      "|26-35|     F|\n",
      "| 0-17|     M|\n",
      "|36-45|     F|\n",
      "|51-55|     M|\n",
      "|26-35|     M|\n",
      "|46-50|     F|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To get the DataFrame without any duplicate rows of given a DataFrame\n",
    "## Use dropDuplicates operation to drop the duplicate rows of a DataFrame. \n",
    "## In this command, performing this on two columns Age and Gender of train dataset and \n",
    "## Get the all unique rows for these two columns.\n",
    "trainDF.select('Age','Gender').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Nulls in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166821\n",
      "166821\n",
      "166821\n"
     ]
    }
   ],
   "source": [
    "## To drop the all rows with null value?\n",
    "## Use dropna operation. \n",
    "## To drop row from the DataFrame it consider three options.\n",
    "## how – ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "\n",
    "## thresh – int, default None If specified, drop rows that have less than thresh non-null values. \n",
    "## This overwrites the how parameter.\n",
    "\n",
    "## subset – optional list of column names to consider.\n",
    "\n",
    "##Drop null rows in train with default parameters and count the rows in output DataFrame. \n",
    "## Default options are any, None, None for how, thresh, subset respectively.\n",
    "print(trainDF.dropna().count())\n",
    "print(trainDF.na.drop().count())\n",
    "print(trainDF.na.drop(\"any\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              -1.0|              -1.0|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|\n",
      "|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|              -1.0|              -1.0|    1422|\n",
      "|1000001| P00085442|     F|0-17|        10|            A|                         2|             0|                12|              14.0|              -1.0|    1057|\n",
      "|1000002| P00285442|     M| 55+|        16|            C|                        4+|             0|                 8|              -1.0|              -1.0|    7969|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To replace the null values in DataFrame with constant number\n",
    "## Use fillna operation. \n",
    "\n",
    "##The fillna will take two parameters to fill the null values.\n",
    "## value:\n",
    "##     It will take a dictionary to specify which column will replace with which value.\n",
    "##     A value (int , float, string) for all columns.\n",
    "##subset: Specify some selected columns.\n",
    "\n",
    "##Fill ‘-1’ inplace of null values in train DataFrame.\n",
    "trainDF.fillna(-1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550068"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Filling with different values for different columns\n",
    "fill_cols_vals = {\n",
    "\"Gender\": 'M',\n",
    "\"Purchase\" : 999999\n",
    "}\n",
    "trainDF.na.fill(fill_cols_vals).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550068"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.na.replace(\"\", \"UNKNOWN\", \"gender\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n",
      "Count of rows where Purchase Amount more than 15000 are 110523\n"
     ]
    }
   ],
   "source": [
    "## To filter the rows in train dataset which has Purchases more than 15000\n",
    "## apply the filter operation on Purchase column in train DataFrame \n",
    "## to filter out the rows with values more than 15000. \n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\"\n",
    "      .format(trainDF.filter(trainDF.Purchase > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\"\n",
    "      .format(trainDF.filter(col(\"Purchase\") > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\"\n",
    "      .format(trainDF.filter(column(\"Purchase\") > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\"\n",
    "      .format(trainDF.filter(expr(\"Purchase\") > 15000).count()))\n",
    "print(\"Count of rows where Purchase Amount more than 15000 are {}\"\n",
    "      .format(trainDF.filter(trainDF[\"Purchase\"] > 15000).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| Count|\n",
      "+------+\n",
      "|110523|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "COUNT(*) AS Count\n",
    "FROM trainDFTable\n",
    "WHERE Purchase > 15000\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110523"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.where(\"Purchase > 15000\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.where(\"Purchase > 15000\").where(\"Gender = 'F'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.filter(\"Purchase > 15000\").where(\"Gender = 'F'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.where((col(\"Purchase\") > 15000) & (col(\"Gender\") == 'F')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.filter((col(\"Purchase\") > 15000) & (col(\"Gender\") == 'F')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21429"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM trainDFTable WHERE Purchase > 15000 AND Gender = 'F'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT Product_Category_3)|\n",
      "+----------------------------------+\n",
      "|                                15|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(countDistinct(\"Product_Category_3\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate Count Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|approx_count_distinct(Product_Category_3)|\n",
      "+-----------------------------------------+\n",
      "|                                       16|\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "trainDF.select(approx_count_distinct(\"Product_Category_3\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First and Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------+\n",
      "|first(Product_ID, false)|last(Product_ID, false)|\n",
      "+------------------------+-----------------------+\n",
      "|               P00069042|              P00371644|\n",
      "+------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "trainDF.select(first(\"Product_ID\"), last(\"Product_ID\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min and Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Purchase)|max(Purchase)|\n",
      "+-------------+-------------+\n",
      "|           12|        23961|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "trainDF.select(min(\"Purchase\"), max(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Purchase)|\n",
      "+-------------+\n",
      "|   5095812742|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "trainDF.select(sum(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Purchase)|\n",
      "+----------------------+\n",
      "|             208520914|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "trainDF.select(sumDistinct(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "temp = trainDF.select(count(\"Purchase\").alias(\"total_transactions\"),\n",
    "                      sum(\"Purchase\").alias(\"total_purchases\"),\n",
    "                      avg(\"Purchase\").alias(\"avg_purchases\"),\n",
    "                      expr(\"mean(Purchase)\").alias(\"mean_purchases\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------+-----------------+\n",
      "|(total_purchases / total_transactions)|    avg_purchases|   mean_purchases|\n",
      "+--------------------------------------+-----------------+-----------------+\n",
      "|                     9263.968712959126|9263.968712959126|9263.968712959126|\n",
      "+--------------------------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp.selectExpr(\"total_purchases/total_transactions\",\"avg_purchases\",\"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+---------------------+\n",
      "|  var_pop(Purchase)|  var_samp(Purchase)|stddev_pop(Purchase)|stddev_samp(Purchase)|\n",
      "+-------------------+--------------------+--------------------+---------------------+\n",
      "|2.523114008138554E7|2.5231185950597987E7|   5023.060827959935|    5023.065393820589|\n",
      "+-------------------+--------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "\n",
    "trainDF.select(var_pop(\"Purchase\"), var_samp(\"Purchase\"),\n",
    "  stddev_pop(\"Purchase\"), stddev_samp(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------------+-----------------+\n",
      "|            var_pop|            var_samp|          std_pop|          std_var|\n",
      "+-------------------+--------------------+-----------------+-----------------+\n",
      "|2.523114008138554E7|2.5231185950597987E7|5023.060827959935|5023.065393820589|\n",
      "+-------------------+--------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT var_pop(Purchase) as var_pop, var_samp(Purchase) as var_samp,\n",
    "             stddev_pop(Purchase) as std_pop, stddev_samp(Purchase) as std_var\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skewness and kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|skewness(Purchase)| kurtosis(Purchase)|\n",
      "+------------------+-------------------+\n",
      "| 0.600138367164343|-0.3383853975360882|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "trainDF.select(skewness(\"Purchase\"), kurtosis(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|skewness(CAST(Purchase AS DOUBLE))|kurtosis(CAST(Purchase AS DOUBLE))|\n",
      "+----------------------------------+----------------------------------+\n",
      "|                 0.600138367164343|               -0.3383853975360882|\n",
      "+----------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT skewness(Purchase), kurtosis(Purchase)\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+------------------+\n",
      "|                Corr|         Cov_samp|           Cov_pop|\n",
      "+--------------------+-----------------+------------------+\n",
      "|-0.34370334591990637|-6795.65000720455|-6795.637653004693|\n",
      "+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "trainDF.select(corr(\"Product_Category_1\", \"Purchase\").alias('Corr'), \n",
    "               covar_samp(\"Product_Category_1\", \"Purchase\").alias('Cov_samp'),\n",
    "               covar_pop(\"Product_Category_1\", \"Purchase\").alias('Cov_pop')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+------------------+\n",
      "|                Corr|         Cov_samp|           Cov_pop|\n",
      "+--------------------+-----------------+------------------+\n",
      "|-0.34370334591990637|-6795.65000720455|-6795.637653004693|\n",
      "+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT corr(Product_Category_1, Purchase) as Corr, \n",
    "             covar_samp(Product_Category_1, Purchase) as Cov_samp,\n",
    "             covar_pop(Product_Category_1, Purchase) as Cov_pop\n",
    "             FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|    collect_set(Age)|   collect_list(Age)|\n",
      "+--------------------+--------------------+\n",
      "|[55+, 51-55, 0-17...|[0-17, 0-17, 0-17...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "trainDF.agg(collect_set(\"Age\"), collect_list(\"Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|collect_set(Age)                              |\n",
      "+----------------------------------------------+\n",
      "|[55+, 51-55, 0-17, 36-45, 46-50, 18-25, 26-35]|\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "trainDF.agg(collect_set(\"Age\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID,Product_ID,Gender,Age,Occupation,City_Category,Stay_In_Current_City_Years,Marital_Status,Product_Category_1,Product_Category_2,Product_Category_3,Purchase\r\n",
      "1000001,P00069042,F,0-17,10,A,2,0,3,,,8370\r\n",
      "1000001,P00248942,F,0-17,10,A,2,0,1,6.0,14.0,15200\r\n",
      "1000001,P00087842,F,0-17,10,A,2,0,12,,,1422\r\n",
      "1000001,P00085442,F,0-17,10,A,2,0,12,14.0,,1057\r\n",
      "1000002,P00285442,M,55+,16,C,4+,0,8,,,7969\r\n",
      "1000003,P00193542,M,26-35,15,A,3,0,1,2.0,,15227\r\n",
      "1000004,P00184942,M,46-50,7,B,2,1,1,8.0,17.0,19215\r\n",
      "1000004,P00346142,M,46-50,7,B,2,1,1,15.0,,15854\r\n",
      "1000004,P0097242,M,46-50,7,B,2,1,1,16.0,,15686\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 train_sample10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|    collect_set(Age)|   collect_list(Age)|\n",
      "+--------------------+--------------------+\n",
      "|[55+, 51-55, 0-17...|[0-17, 0-17, 0-17...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT collect_set(Age), collect_list(Age) FROM trainDFTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "|  Age|Gender| count|\n",
      "+-----+------+------+\n",
      "|51-55|     F|  9894|\n",
      "|18-25|     M| 75032|\n",
      "| 0-17|     F|  5083|\n",
      "|46-50|     M| 32502|\n",
      "|18-25|     F| 24628|\n",
      "|  55+|     M| 16421|\n",
      "|  55+|     F|  5083|\n",
      "|36-45|     M| 82843|\n",
      "|26-35|     F| 50752|\n",
      "| 0-17|     M| 10019|\n",
      "|36-45|     F| 27170|\n",
      "|51-55|     M| 28607|\n",
      "|26-35|     M|168835|\n",
      "|46-50|     F| 13199|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupBy(\"Age\", \"Gender\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping with Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------------+\n",
      "|  Age|  quan|count(Purchase)|\n",
      "+-----+------+---------------+\n",
      "|18-25| 99660|          99660|\n",
      "|26-35|219587|         219587|\n",
      "| 0-17| 15102|          15102|\n",
      "|46-50| 45701|          45701|\n",
      "|51-55| 38501|          38501|\n",
      "|36-45|110013|         110013|\n",
      "|  55+| 21504|          21504|\n",
      "+-----+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.groupBy(\"Age\") \\\n",
    "       .agg(count(\"Purchase\").alias(\"quan\"),\n",
    "             expr(\"count(Purchase)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+--------------------+\n",
      "|  Age|    avg(Purchase)|stddev_pop(Purchase)|\n",
      "+-----+-----------------+--------------------+\n",
      "|18-25|9169.663606261289|    5034.29673962778|\n",
      "|26-35|9252.690632869888|   5010.515894010147|\n",
      "| 0-17|8933.464640444974|   5110.944823427656|\n",
      "|46-50|9208.625697468327|     4967.1620221227|\n",
      "|51-55|9534.808030960236|    5087.30201117386|\n",
      "|36-45|9331.350694917874|   5022.901050378549|\n",
      "|  55+|9336.280459449405|   5011.377469555765|\n",
      "+-----+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "trainDF.groupBy(\"Age\").agg(expr(\"avg(Purchase)\"),expr(\"stddev_pop(Purchase)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|  Age|    avg(Purchase)|\n",
      "+-----+-----------------+\n",
      "|18-25|9169.663606261289|\n",
      "|26-35|9252.690632869888|\n",
      "| 0-17|8933.464640444974|\n",
      "|46-50|9208.625697468327|\n",
      "|51-55|9534.808030960236|\n",
      "|36-45|9331.350694917874|\n",
      "|  55+|9336.280459449405|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To find the mean of each age group in train dataset - Average purchases in each age group\n",
    "trainDF.groupby('Age').agg({'Purchase': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|  Age|sum(Purchase)|\n",
      "+-----+-------------+\n",
      "|18-25|    913848675|\n",
      "|26-35|   2031770578|\n",
      "| 0-17|    134913183|\n",
      "|46-50|    420843403|\n",
      "|51-55|    367099644|\n",
      "|36-45|   1026569884|\n",
      "|  55+|    200767375|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To find Sum of purchases in each age group in the train dataset\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------------+-------------------+-------------+------------+---------------+-------------------------------+-----------------------+--------+-----------+-----------------------+---------------+\n",
      "|  Age|sum(City_Category)|sum(Product_Category_3)|sum(Marital_Status)|sum(Purchase)|sum(User_ID)|sum(Occupation)|sum(Stay_In_Current_City_Years)|sum(Product_Category_1)|sum(Age)|sum(Gender)|sum(Product_Category_2)|sum(Product_ID)|\n",
      "+-----+------------------+-----------------------+-------------------+-------------+------------+---------------+-------------------------------+-----------------------+--------+-----------+-----------------------+---------------+\n",
      "|18-25|              null|               388041.0|              21116|    913848675| 99939196632|         671348|                       116997.0|                 509371|    null|       null|               654936.0|           null|\n",
      "|26-35|              null|               846624.0|              86291|   2031770578|220270500414|        1734073|                       275611.0|                1166945|    null|       null|              1473278.0|           null|\n",
      "| 0-17|              null|                57725.0|                  0|    134913183| 15143112813|         132309|                        20320.0|                  76775|    null|       null|                96155.0|           null|\n",
      "|46-50|              null|               173059.0|              33011|    420843403| 45846804203|         389239|                        51742.0|                 262424|    null|       null|               315572.0|           null|\n",
      "|51-55|              null|               146334.0|              27662|    367099644| 38615925320|         339198|                        44243.0|                 222313|    null|       null|               267570.0|           null|\n",
      "|36-45|              null|               424412.0|              43636|   1026569884|110350311441|         972225|                       148162.0|                 604438|    null|       null|               750081.0|           null|\n",
      "|  55+|              null|                77134.0|              13621|    200767375| 21568218459|         204346|                        26277.0|                 130450|    null|       null|               147356.0|           null|\n",
      "+-----+------------------+-----------------------+-------------------+-------------+------------+---------------+-------------------------------+-----------------------+--------+-----------+-----------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Apply sum, min, max, count with groupby to get different summary insight for each group. \n",
    "exprs = {x: \"sum\" for x in trainDF.columns}\n",
    "trainDF.groupBy(\"Age\").agg(exprs).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+\n",
      "| id|             name|graduate_program|    role_status|\n",
      "+---+-----------------+----------------+---------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  2|        Dr. Manoj|               2|          [100]|\n",
      "+---+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "person = spark.createDataFrame([\n",
    "    (0, \"Dr. Murthy\", 0, [250, 100]),\n",
    "    (1, \"Dr. Sridhar Pappu\", 1, [500, 250, 100]),\n",
    "    (2, \"Dr. Manoj\", 2, [100])], [\"id\", \"name\", \"graduate_program\", \"role_status\"])\n",
    "\n",
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------------------+--------------------------+\n",
      "|id |degree|department                  |school                    |\n",
      "+---+------+----------------------------+--------------------------+\n",
      "|0  |Ph.D  |School of Information       |Carnegie Mellon University|\n",
      "|1  |Ph.D  |The University of Texas     |El Paso                   |\n",
      "|2  |Ph.D. |School of Information       |Oklahoma State University |\n",
      "|3  |Ph.D  |The University of California|Berkeley                  |\n",
      "+---+------+----------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Ph.D\", \"School of Information\", \"Carnegie Mellon University\"),\n",
    "    (1, \"Ph.D\", \"The University of Texas\", \"El Paso\"),\n",
    "    (2, \"Ph.D.\", \"School of Information\", \"Oklahoma State University\"),\n",
    "    (3, \"Ph.D\", \"The University of California\", \"Berkeley\")], [\"id\", \"degree\", \"department\", \"school\"])\n",
    "graduateProgram.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|   status|\n",
      "+---+---------+\n",
      "|500|President|\n",
      "|250|  Founder|\n",
      "|100|   Mentor|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roleStatus = spark.createDataFrame([\n",
    "    (500, \"President\"),\n",
    "    (250, \"Founder\"),\n",
    "    (100, \"Mentor\")], [\"id\", \"status\"])\n",
    "roleStatus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "person.createOrReplaceTempView(\"personTbl\")\n",
    "graduateProgram.?\n",
    "roleStatus.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "spark.sql(\"\"\"SELECT * FROM personTbl JOIN graduateProgramTbl\n",
    "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"inner\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM personTbl INNER JOIN graduateProgramTbl\n",
    "  ON personTbl.graduate_program = graduateProgramTbl.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|   0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|   1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|null|             null|            null|           null|  3|  Ph.D|The University of...|            Berkeley|\n",
      "|   2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|   0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|   1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|null|             null|            null|           null|  3|  Ph.D|The University of...|            Berkeley|\n",
      "|   2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left Outer Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+--------------------+----+-----------------+----------------+---------------+\n",
      "| id|degree|          department|              school|  id|             name|graduate_program|    role_status|\n",
      "+---+------+--------------------+--------------------+----+-----------------+----------------+---------------+\n",
      "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|   0|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  1|  Ph.D|The University of...|             El Paso|   1|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  3|  Ph.D|The University of...|            Berkeley|null|             null|            null|           null|\n",
      "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|   2|        Dr. Manoj|               2|          [100]|\n",
      "+---+------+--------------------+--------------------+----+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Right Outer Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|   0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|   1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|null|             null|            null|           null|  3|  Ph.D|The University of...|            Berkeley|\n",
      "|   2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|   0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|   1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|null|             null|            null|           null|  3|  Ph.D|The University of...|            Berkeley|\n",
      "|   2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "+----+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Joins\n",
    "Natural joins make implicit guesses at the columns on which you would like to join. \n",
    "It finds matching columns and returns the results. \n",
    "Left, right, and outer natural joins are all supported.\n",
    "\n",
    "WARNING:\n",
    "Implicit is always dangerous! \n",
    "The following query will give us incorrect results because \n",
    "the two DataFrames/tables share a column name (id), but it means different things in the datasets. \n",
    "You should always use this join with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "| id|degree|          department|              school|             name|graduate_program|    role_status|\n",
      "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|       Dr. Murthy|               0|     [250, 100]|\n",
      "|  1|  Ph.D|The University of...|             El Paso|Dr. Sridhar Pappu|               1|[500, 250, 100]|\n",
      "|  2| Ph.D.|School of Informa...|Oklahoma State Un...|        Dr. Manoj|               2|          [100]|\n",
      "+---+------+--------------------+--------------------+-----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM graduateProgramTbl NATURAL JOIN personTbl\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross (Cartesian) Joins\n",
    "Cross-joins in simplest terms are inner joins that do not specify a predicate. \n",
    "Cross joins will join every single row in the left DataFrame to ever single row in the right DataFrame. \n",
    "This will cause an absolute explosion in the number of rows contained in the resulting DataFrame. \n",
    "If you have 1,000 rows in each DataFrame, the cross-join of these will result in 1,000,000 (1,000 x 1,000) rows. \n",
    "For this reason, you must very explicitly state that you want a cross-join by using the cross join keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  3|  Ph.D|The University of...|            Berkeley|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  3|  Ph.D|The University of...|            Berkeley|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  3|  Ph.D|The University of...|            Berkeley|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "| id|             name|graduate_program|    role_status| id|degree|          department|              school|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  0|       Dr. Murthy|               0|     [250, 100]|  3|  Ph.D|The University of...|            Berkeley|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  1|Dr. Sridhar Pappu|               1|[500, 250, 100]|  3|  Ph.D|The University of...|            Berkeley|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  0|  Ph.D|School of Informa...|Carnegie Mellon U...|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  1|  Ph.D|The University of...|             El Paso|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  2| Ph.D.|School of Informa...|Oklahoma State Un...|\n",
      "|  2|        Dr. Manoj|               2|          [100]|  3|  Ph.D|The University of...|            Berkeley|\n",
      "+---+-----------------+----------------+---------------+---+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM personTbl CROSS JOIN graduateProgramTbl\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins on Complex Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "|personId|             name|graduate_program|    role_status| id|   status|\n",
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "|       0|       Dr. Murthy|               0|     [250, 100]|250|  Founder|\n",
      "|       0|       Dr. Murthy|               0|     [250, 100]|100|   Mentor|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|500|President|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|250|  Founder|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|100|   Mentor|\n",
      "|       2|        Dr. Manoj|               2|          [100]|100|   Mentor|\n",
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "person.withColumnRenamed(\"id\", \"personId\")\\\n",
    "  .join(roleStatus, expr(\"array_contains(role_status, id)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "|personId|             name|graduate_program|    role_status| id|   status|\n",
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "|       0|       Dr. Murthy|               0|     [250, 100]|250|  Founder|\n",
      "|       0|       Dr. Murthy|               0|     [250, 100]|100|   Mentor|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|500|President|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|250|  Founder|\n",
      "|       1|Dr. Sridhar Pappu|               1|[500, 250, 100]|100|   Mentor|\n",
      "|       2|        Dr. Manoj|               2|          [100]|100|   Mentor|\n",
      "+--------+-----------------+----------------+---------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM\n",
    "  (select id as personId, name, graduate_program, role_status FROM personTbl)\n",
    "  INNER JOIN roleStatusTbl ON array_contains(role_status, id)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110200, 110209)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## To create a sample DataFrame from the base DataFrame\n",
    "## Use sample operation to take sample of a DataFrame. \n",
    "## The sample method on DataFrame will return a DataFrame containing the sample of base DataFrame. \n",
    "## The sample method takes 3 parameters.\n",
    "## withReplacement = True or False to select a observation with or without replacement.\n",
    "## fraction = x, where x = .5 shows that we want to have 50% data in sample DataFrame.\n",
    "## seed to reproduce the result\n",
    "sampleDF1 = trainDF.sample(False, 0.2, 1234)\n",
    "sampleDF2 = ?\n",
    "print(sampleDF1.count(), sampleDF2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000004| P00184942|     M|46-50|         7|            B|                         2|             1|                 1|               8.0|              17.0|   19215|\n",
      "|1000005| P00014542|     M|26-35|        20|            A|                         1|             1|                 8|              null|              null|    3957|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleDF1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385469\n",
      "164599\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "splitDF = trainDF.?\n",
    "print(splitDF[0].count())\n",
    "print(splitDF[1].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1001474| P00052842|     M|26-35|         4|            A|                         2|             1|                10|              15.0|              null|   23961|\n",
      "|1002272| P00052842|     M|26-35|         0|            C|                         1|             0|                10|              15.0|              null|   23961|\n",
      "|1003160| P00052842|     M|26-35|        17|            C|                         3|             0|                10|              15.0|              null|   23961|\n",
      "|1001577| P00052842|     M|  55+|         0|            C|                         1|             1|                10|              15.0|              null|   23960|\n",
      "|1005596| P00117642|     M|36-45|        12|            B|                         1|             0|                10|              16.0|              null|   23960|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To sort the DataFrame based on column(s)\n",
    "## Use orderBy operation on DataFrame to get sorted output based on some column. \n",
    "## The orderBy operation take two arguments.\n",
    "## List of columns.\n",
    "## ascending = True or False for getting the results in ascending or descending order(list in case of more than two columns )\n",
    "## Sort the train DataFrame based on ‘Purchase’.\n",
    "trainDF.orderBy(trainDF.Purchase.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repartition and Coalesce\n",
    "Another important optimization opportunity is to partition the data according to some frequently filtered columns\n",
    "which controls the physical layout of data across the cluster including the partitioning scheme and the number of\n",
    "partitions.\n",
    "\n",
    "Repartition will incur a full shuffle of the data, regardless of whether or not one is necessary. This means that you should typically only repartition when the future number of partitions is greater than your current number of\n",
    "partitions or when you are looking to partition by a set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Find existing partitions count\n",
    "trainDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Do repartition\n",
    "trainDF1 = trainDF.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT Product_Category_3)|\n",
      "+----------------------------------+\n",
      "|                                15|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Repartition based on a column\n",
    "## If we know we are going to be filtering by a certain column often, \n",
    "## it can be worth repartitioning based on that column.\n",
    "\n",
    "trainDF.select(countDistinct(\"Product_Category_3\")).show()\n",
    "trainDF_1 = trainDF.repartition(15, \"Product_Category_3\")\n",
    "trainDF_1.rdd.getNumPartitions()\n",
    "\n",
    "#trainDF.describe(trainDF.Product_Category_3)\n",
    "\n",
    "## We can optionally specify the number of partitions we would like too.\n",
    "## trainDF.repartition(5, col(\"Product_Category_3\"))\n",
    "\n",
    "## Coalesce on the other hand will not incur a full shuffle and will try to combine partitions. \n",
    "## This operation will shuffle our data into 5 partitions based on the Purchase, \n",
    "## then coalesce them (without a full shuffle).\n",
    "## trainDF.repartition(5, col(\"Purchase\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|              null|              null|    8370|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|               6.0|              14.0|   15200|\n",
      "|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|              null|              null|    1422|\n",
      "|1000001| P00085442|     F|0-17|        10|            A|                         2|             0|                12|              14.0|              null|    1057|\n",
      "|1000002| P00285442|     M| 55+|        16|            C|                        4+|             0|                 8|              null|              null|    7969|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "DataFrame-1\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|   Alex| 25|\n",
      "|  3|  Carol| 53|\n",
      "|  5|  Emily| 25|\n",
      "|  7|Gabriel| 32|\n",
      "|  9|   Ilma| 35|\n",
      "| 11|    Kim| 45|\n",
      "+---+-------+---+\n",
      "\n",
      "None\n",
      "DataFrame-2\n",
      "+---+------+---+\n",
      "| id|  name|age|\n",
      "+---+------+---+\n",
      "|  2|   Ben| 66|\n",
      "|  4|Daniel| 28|\n",
      "|  6| Frank| 64|\n",
      "|  8|Harley| 29|\n",
      "| 10|  Jack| 35|\n",
      "| 12|Litmya| 45|\n",
      "+---+------+---+\n",
      "\n",
      "None\n",
      "After Union\n",
      "DataFrame-1\n",
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|   Alex| 25|\n",
      "|  3|  Carol| 53|\n",
      "|  5|  Emily| 25|\n",
      "|  7|Gabriel| 32|\n",
      "|  9|   Ilma| 35|\n",
      "| 11|    Kim| 45|\n",
      "+---+-------+---+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([[1, 'Alex', 25],[3, 'Carol', 53],[5, 'Emily', 25],[7, 'Gabriel', 32],[9, 'Ilma', 35],[11, 'Kim', 45]], ['id', 'name', 'age'])\n",
    "df2 = spark.createDataFrame([[2, 'Ben', 66],[4, 'Daniel', 28],[6, 'Frank', 64],[8, 'Harley', 29],[10, 'Jack', 35],[12, 'Litmya', 45]], ['id', 'name', 'age'])\n",
    "print(\"Before\")\n",
    "print(\"DataFrame-1\")\n",
    "print(df1.show())\n",
    "print(\"DataFrame-2\")\n",
    "print(df2.show())\n",
    "print(\"After Union\")\n",
    "df3 = df1.union(df2)\n",
    "print(\"DataFrame-1\")\n",
    "print(df1.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unions and conditional append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|   Alex| 25|\n",
      "|  3|  Carol| 53|\n",
      "|  5|  Emily| 25|\n",
      "|  7|Gabriel| 32|\n",
      "|  9|   Ilma| 35|\n",
      "| 11|    Kim| 45|\n",
      "|  4| Daniel| 28|\n",
      "|  8| Harley| 29|\n",
      "| 10|   Jack| 35|\n",
      "| 12| Litmya| 45|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).where(\"age < 60\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|Purchase|Purchase_new|\n",
      "+--------+------------+\n",
      "|    8370|      4185.0|\n",
      "|   15200|      7600.0|\n",
      "|    1422|       711.0|\n",
      "|    1057|       528.5|\n",
      "|    7969|      3984.5|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To add the new column in DataFrame\n",
    "## Use withColumn operation to add new column (we can also replace) in base DataFrame and return a new DataFrame. \n",
    "## The withColumn operation will take 2 parameters.\n",
    "## Column name to be added /replaced.\n",
    "## Expression on column.\n",
    "\n",
    "## Derive new column, ‘Purchase_new’ in train which is calculated by dviding Purchase column by 2.\n",
    "\n",
    "trainDF.withColumn('Purchase_new', trainDF.Purchase /2.0).select('Purchase','Purchase_new').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Product_ID',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Occupation',\n",
       " 'City_Category',\n",
       " 'Stay_In_Current_City_Years',\n",
       " 'Marital_Status',\n",
       " 'Product_Category_1',\n",
       " 'Product_Category_2',\n",
       " 'Product_Category_3']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To drop a column in DataFrame\n",
    "## To drop a column from the DataFrame use drop operation. \n",
    "## Drop the column called ‘Comb’ from the test and get the remaining columns in test dataframe\n",
    "testDF.drop('Comb').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To remove some categories of Product_ID column in test that are not present in Product_ID column in train\n",
    "## Use an user defined function ( udf ) to remove the categories of a column which are in test but not in train.\n",
    "## Calculate the categories in Product_ID column which are in test but not in train.\n",
    "diff_cat_in_test_train = testDF.select('Product_ID').subtract(trainDF.select('Product_ID'))\n",
    "diff_cat_in_test_train.count() # For distinct count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Product_ID|\n",
      "+----------+\n",
      "| P00074942|\n",
      "| P00030342|\n",
      "| P00279042|\n",
      "| P00140842|\n",
      "| P00058842|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff_cat_in_test_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<type 'list'>\n",
      "[u'P00074942', u'P00030342', u'P00279042', u'P00140842', u'P00058842']\n"
     ]
    }
   ],
   "source": [
    "## There are 5 different categories in test. \n",
    "## To remove these categories from the test ‘Product_ID’ column.\n",
    "\n",
    "## Create the distinct list of categories called ‘not_found_cat’ from the diff_cat_in_test_train using map operation.\n",
    "## Register a udf(user define function).\n",
    "## User defined function will take each element of test column and search this in not_found_cat list and \n",
    "## it will put -1 ifit finds in this list otherwise it will do nothing.\n",
    "not_found_cat = diff_cat_in_test_train.rdd.map(lambda x: x[0]).collect()\n",
    "print(len(not_found_cat))\n",
    "print(type(not_found_cat))\n",
    "print(not_found_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Defined Functions - UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Register the udf, we need to import StringType from the pyspark.sql and udf from the pyspark.sql.functions. \n",
    "## The udf function takes 2 parameters as arguments:\n",
    "## Return type (in my case StringType())\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "Function1 = udf(lambda x: '-1' if x in not_found_cat else x, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|NEW_Product_ID|\n",
      "+--------------+\n",
      "|            -1|\n",
      "|            -1|\n",
      "|            -1|\n",
      "|            -1|\n",
      "|            -1|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## In the above code function name is ‘Function1’ and we are putting ‘-1’  for not found catagories in test ‘Product_ID’. \n",
    "## Finally apply above ‘Function1’ function on test ‘Product_ID’ and take result in k for new column calles “NEW_Product_ID”.\n",
    "\n",
    "k = testDF.withColumn(\"NEW_Product_ID\",Function1(testDF[\"Product_ID\"])).select('NEW_Product_ID')\n",
    "k.where(k['NEW_Product_ID'] == -1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## See the results by again calculating the different categories in k and train subtract operation.\n",
    "diff_cat_in_test_train=k.select('NEW_Product_ID').subtract(trainDF.select('Product_ID'))\n",
    "print(diff_cat_in_test_train.count())# For distinct count\n",
    "print(diff_cat_in_test_train.distinct().count())# For distinct count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Product_ID=u'P00122142'),\n",
       " Row(Product_ID=u'P00162742'),\n",
       " Row(Product_ID=u'P00256142'),\n",
       " Row(Product_ID=u'P00323242'),\n",
       " Row(Product_ID=u'P00063742'),\n",
       " Row(Product_ID=u'P00068142'),\n",
       " Row(Product_ID=u'P00132142'),\n",
       " Row(Product_ID=u'P00132742'),\n",
       " Row(Product_ID=u'P00164742'),\n",
       " Row(Product_ID=u'P00368542'),\n",
       " Row(Product_ID=u'P00027742'),\n",
       " Row(Product_ID=u'P00066142'),\n",
       " Row(Product_ID=u'P00068542'),\n",
       " Row(Product_ID=u'P00139442'),\n",
       " Row(Product_ID=u'P00236242'),\n",
       " Row(Product_ID=u'P00311042'),\n",
       " Row(Product_ID=u'P00172642'),\n",
       " Row(Product_ID=u'P00262042'),\n",
       " Row(Product_ID=u'P00033242'),\n",
       " Row(Product_ID=u'P00080942'),\n",
       " Row(Product_ID=u'P00229442'),\n",
       " Row(Product_ID=u'P00338242'),\n",
       " Row(Product_ID=u'P00062642'),\n",
       " Row(Product_ID=u'P00160742'),\n",
       " Row(Product_ID=u'P00201942'),\n",
       " Row(Product_ID=u'P00036642'),\n",
       " Row(Product_ID=u'P00076942'),\n",
       " Row(Product_ID=u'P00080742'),\n",
       " Row(Product_ID=u'P00136342'),\n",
       " Row(Product_ID=u'P00163942'),\n",
       " Row(Product_ID=u'P00203942'),\n",
       " Row(Product_ID=u'P00242242'),\n",
       " Row(Product_ID=u'P00371644'),\n",
       " Row(Product_ID=u'P00065842'),\n",
       " Row(Product_ID=u'P00100542'),\n",
       " Row(Product_ID=u'P00138942'),\n",
       " Row(Product_ID=u'P00182942'),\n",
       " Row(Product_ID=u'P00203642'),\n",
       " Row(Product_ID=u'P00360742'),\n",
       " Row(Product_ID=u'P00364042'),\n",
       " Row(Product_ID=u'P00056542'),\n",
       " Row(Product_ID=u'P00092342'),\n",
       " Row(Product_ID=u'P00206542'),\n",
       " Row(Product_ID=u'P00206942'),\n",
       " Row(Product_ID=u'P00054742'),\n",
       " Row(Product_ID=u'P00065542'),\n",
       " Row(Product_ID=u'P00325642'),\n",
       " Row(Product_ID=u'P00329142'),\n",
       " Row(Product_ID=u'P00010642'),\n",
       " Row(Product_ID=u'P00062342'),\n",
       " Row(Product_ID=u'P00131642'),\n",
       " Row(Product_ID=u'P00212642'),\n",
       " Row(Product_ID=u'P00011142'),\n",
       " Row(Product_ID=u'P00107942'),\n",
       " Row(Product_ID=u'P00190642'),\n",
       " Row(Product_ID=u'P00309642'),\n",
       " Row(Product_ID=u'P00340742'),\n",
       " Row(Product_ID=u'P00078442'),\n",
       " Row(Product_ID=u'P00160842'),\n",
       " Row(Product_ID=u'P00232142'),\n",
       " Row(Product_ID=u'P00253642'),\n",
       " Row(Product_ID=u'P00073142'),\n",
       " Row(Product_ID=u'P00363042'),\n",
       " Row(Product_ID=u'P00043642'),\n",
       " Row(Product_ID=u'P00062942'),\n",
       " Row(Product_ID=u'P00071042'),\n",
       " Row(Product_ID=u'P00151642'),\n",
       " Row(Product_ID=u'P00307742'),\n",
       " Row(Product_ID=u'P00061442'),\n",
       " Row(Product_ID=u'P00067742'),\n",
       " Row(Product_ID=u'P00201242'),\n",
       " Row(Product_ID=u'P00211542'),\n",
       " Row(Product_ID=u'P00295142'),\n",
       " Row(Product_ID=u'P00330142'),\n",
       " Row(Product_ID=u'P00022042'),\n",
       " Row(Product_ID=u'P00025742'),\n",
       " Row(Product_ID=u'P00152342'),\n",
       " Row(Product_ID=u'P00201642'),\n",
       " Row(Product_ID=u'P00227442'),\n",
       " Row(Product_ID=u'P00310742'),\n",
       " Row(Product_ID=u'P00312442'),\n",
       " Row(Product_ID=u'P00032242'),\n",
       " Row(Product_ID=u'P00204142'),\n",
       " Row(Product_ID=u'P0098942'),\n",
       " Row(Product_ID=u'P00055442'),\n",
       " Row(Product_ID=u'P00061242'),\n",
       " Row(Product_ID=u'P00071842'),\n",
       " Row(Product_ID=u'P00122742'),\n",
       " Row(Product_ID=u'P00144542'),\n",
       " Row(Product_ID=u'P00264842'),\n",
       " Row(Product_ID=u'P00086542'),\n",
       " Row(Product_ID=u'P00167242'),\n",
       " Row(Product_ID=u'P00169842'),\n",
       " Row(Product_ID=u'P00234042'),\n",
       " Row(Product_ID=u'P00235042'),\n",
       " Row(Product_ID=u'P00269142'),\n",
       " Row(Product_ID=u'P00107842'),\n",
       " Row(Product_ID=u'P00138242'),\n",
       " Row(Product_ID=u'P00198442'),\n",
       " Row(Product_ID=u'P00204042'),\n",
       " Row(Product_ID=u'P00234442'),\n",
       " Row(Product_ID=u'P00022242'),\n",
       " Row(Product_ID=u'P00060842'),\n",
       " Row(Product_ID=u'P00137842'),\n",
       " Row(Product_ID=u'P00010442'),\n",
       " Row(Product_ID=u'P00065942'),\n",
       " Row(Product_ID=u'P00081242'),\n",
       " Row(Product_ID=u'P00150242'),\n",
       " Row(Product_ID=u'P00342842'),\n",
       " Row(Product_ID=u'P00012542'),\n",
       " Row(Product_ID=u'P00077542'),\n",
       " Row(Product_ID=u'P00139042'),\n",
       " Row(Product_ID=u'P00143042'),\n",
       " Row(Product_ID=u'P00353042'),\n",
       " Row(Product_ID=u'P00046542'),\n",
       " Row(Product_ID=u'P00083142'),\n",
       " Row(Product_ID=u'P00153342'),\n",
       " Row(Product_ID=u'P00305742'),\n",
       " Row(Product_ID=u'P00317342'),\n",
       " Row(Product_ID=u'P00060942'),\n",
       " Row(Product_ID=u'P00197642'),\n",
       " Row(Product_ID=u'P00245542'),\n",
       " Row(Product_ID=u'P00277042'),\n",
       " Row(Product_ID=u'P00123342'),\n",
       " Row(Product_ID=u'P00175342'),\n",
       " Row(Product_ID=u'P00329842'),\n",
       " Row(Product_ID=u'P00100342'),\n",
       " Row(Product_ID=u'P00238642'),\n",
       " Row(Product_ID=u'P00257342'),\n",
       " Row(Product_ID=u'P00290542'),\n",
       " Row(Product_ID=u'P00349142'),\n",
       " Row(Product_ID=u'P00369542'),\n",
       " Row(Product_ID=u'P00080842'),\n",
       " Row(Product_ID=u'P00107042'),\n",
       " Row(Product_ID=u'P00214742'),\n",
       " Row(Product_ID=u'P00260742'),\n",
       " Row(Product_ID=u'P00143442'),\n",
       " Row(Product_ID=u'P00266842'),\n",
       " Row(Product_ID=u'P00276742'),\n",
       " Row(Product_ID=u'P00278542'),\n",
       " Row(Product_ID=u'P00281342'),\n",
       " Row(Product_ID=u'P00341642'),\n",
       " Row(Product_ID=u'P00091742'),\n",
       " Row(Product_ID=u'P00147042'),\n",
       " Row(Product_ID=u'P00162542'),\n",
       " Row(Product_ID=u'P00278842'),\n",
       " Row(Product_ID=u'P00039542'),\n",
       " Row(Product_ID=u'P00068842'),\n",
       " Row(Product_ID=u'P00126742'),\n",
       " Row(Product_ID=u'P00167142'),\n",
       " Row(Product_ID=u'P00242442'),\n",
       " Row(Product_ID=u'P00250142'),\n",
       " Row(Product_ID=u'P00290242'),\n",
       " Row(Product_ID=u'P00048642'),\n",
       " Row(Product_ID=u'P00082942'),\n",
       " Row(Product_ID=u'P00087942'),\n",
       " Row(Product_ID=u'P00127142'),\n",
       " Row(Product_ID=u'P00147842'),\n",
       " Row(Product_ID=u'P00283042'),\n",
       " Row(Product_ID=u'P00038542'),\n",
       " Row(Product_ID=u'P00111342'),\n",
       " Row(Product_ID=u'P00184742'),\n",
       " Row(Product_ID=u'P00318842'),\n",
       " Row(Product_ID=u'P00336242'),\n",
       " Row(Product_ID=u'P00342442'),\n",
       " Row(Product_ID=u'P00350742'),\n",
       " Row(Product_ID=u'P00358642'),\n",
       " Row(Product_ID=u'P00364642'),\n",
       " Row(Product_ID=u'P00103142'),\n",
       " Row(Product_ID=u'P00288842'),\n",
       " Row(Product_ID=u'P00314842'),\n",
       " Row(Product_ID=u'P00110242'),\n",
       " Row(Product_ID=u'P00068342'),\n",
       " Row(Product_ID=u'P00144342'),\n",
       " Row(Product_ID=u'P00171142'),\n",
       " Row(Product_ID=u'P00194842'),\n",
       " Row(Product_ID=u'P00245142'),\n",
       " Row(Product_ID=u'P00081742'),\n",
       " Row(Product_ID=u'P00365942'),\n",
       " Row(Product_ID=u'P00372445'),\n",
       " Row(Product_ID=u'P00136642'),\n",
       " Row(Product_ID=u'P00189942'),\n",
       " Row(Product_ID=u'P00080442'),\n",
       " Row(Product_ID=u'P00087742'),\n",
       " Row(Product_ID=u'P00106542'),\n",
       " Row(Product_ID=u'P00185042'),\n",
       " Row(Product_ID=u'P00249342'),\n",
       " Row(Product_ID=u'P00257942'),\n",
       " Row(Product_ID=u'P00267942'),\n",
       " Row(Product_ID=u'P00341542'),\n",
       " Row(Product_ID=u'P00011642'),\n",
       " Row(Product_ID=u'P00090342'),\n",
       " Row(Product_ID=u'P00186542'),\n",
       " Row(Product_ID=u'P00008642'),\n",
       " Row(Product_ID=u'P00192142'),\n",
       " Row(Product_ID=u'P00275242'),\n",
       " Row(Product_ID=u'P00316042'),\n",
       " Row(Product_ID=u'P00138742'),\n",
       " Row(Product_ID=u'P00243642'),\n",
       " Row(Product_ID=u'P00284542'),\n",
       " Row(Product_ID=u'P00305242'),\n",
       " Row(Product_ID=u'P00039342'),\n",
       " Row(Product_ID=u'P00170242'),\n",
       " Row(Product_ID=u'P00128642'),\n",
       " Row(Product_ID=u'P00165242'),\n",
       " Row(Product_ID=u'P00291642'),\n",
       " Row(Product_ID=u'P00139642'),\n",
       " Row(Product_ID=u'P00019242'),\n",
       " Row(Product_ID=u'P00280942'),\n",
       " Row(Product_ID=u'P00340842'),\n",
       " Row(Product_ID=u'P00356342'),\n",
       " Row(Product_ID=u'P00020742'),\n",
       " Row(Product_ID=u'P00038642'),\n",
       " Row(Product_ID=u'P00044942'),\n",
       " Row(Product_ID=u'P00063642'),\n",
       " Row(Product_ID=u'P00070842'),\n",
       " Row(Product_ID=u'P00235142'),\n",
       " Row(Product_ID=u'P00283442'),\n",
       " Row(Product_ID=u'P00306942'),\n",
       " Row(Product_ID=u'P00356642'),\n",
       " Row(Product_ID=u'P00075042'),\n",
       " Row(Product_ID=u'P00140642'),\n",
       " Row(Product_ID=u'P00265942'),\n",
       " Row(Product_ID=u'P00300242'),\n",
       " Row(Product_ID=u'P00156642'),\n",
       " Row(Product_ID=u'P00298842'),\n",
       " Row(Product_ID=u'P00332542'),\n",
       " Row(Product_ID=u'P00157042'),\n",
       " Row(Product_ID=u'P00166442'),\n",
       " Row(Product_ID=u'P00210742'),\n",
       " Row(Product_ID=u'P00249242'),\n",
       " Row(Product_ID=u'P00267442'),\n",
       " Row(Product_ID=u'P00287742'),\n",
       " Row(Product_ID=u'P00301342'),\n",
       " Row(Product_ID=u'P00093142'),\n",
       " Row(Product_ID=u'P00340042'),\n",
       " Row(Product_ID=u'P00153042'),\n",
       " Row(Product_ID=u'P00245742'),\n",
       " Row(Product_ID=u'P00338642'),\n",
       " Row(Product_ID=u'P00365842'),\n",
       " Row(Product_ID=u'P00025842'),\n",
       " Row(Product_ID=u'P00231842'),\n",
       " Row(Product_ID=u'P00368942'),\n",
       " Row(Product_ID=u'P00064542'),\n",
       " Row(Product_ID=u'P00079542'),\n",
       " Row(Product_ID=u'P00144842'),\n",
       " Row(Product_ID=u'P00340442'),\n",
       " Row(Product_ID=u'P00342342'),\n",
       " Row(Product_ID=u'P00056642'),\n",
       " Row(Product_ID=u'P00065742'),\n",
       " Row(Product_ID=u'P00012742'),\n",
       " Row(Product_ID=u'P00023242'),\n",
       " Row(Product_ID=u'P00160442'),\n",
       " Row(Product_ID=u'P00172442'),\n",
       " Row(Product_ID=u'P00237842'),\n",
       " Row(Product_ID=u'P00339642'),\n",
       " Row(Product_ID=u'P00366842'),\n",
       " Row(Product_ID=u'P00024542'),\n",
       " Row(Product_ID=u'P00073042'),\n",
       " Row(Product_ID=u'P00126342'),\n",
       " Row(Product_ID=u'P00169242'),\n",
       " Row(Product_ID=u'P00170742'),\n",
       " Row(Product_ID=u'P00214942'),\n",
       " Row(Product_ID=u'P00338142'),\n",
       " Row(Product_ID=u'P00045742'),\n",
       " Row(Product_ID=u'P00107542'),\n",
       " Row(Product_ID=u'P00121842'),\n",
       " Row(Product_ID=u'P00149942'),\n",
       " Row(Product_ID=u'P00151542'),\n",
       " Row(Product_ID=u'P00139242'),\n",
       " Row(Product_ID=u'P00353242'),\n",
       " Row(Product_ID=u'P00013542'),\n",
       " Row(Product_ID=u'P00020942'),\n",
       " Row(Product_ID=u'P00291142'),\n",
       " Row(Product_ID=u'P00161042'),\n",
       " Row(Product_ID=u'P00045042'),\n",
       " Row(Product_ID=u'P00131442'),\n",
       " Row(Product_ID=u'P00149042'),\n",
       " Row(Product_ID=u'P00216642'),\n",
       " Row(Product_ID=u'P00305442'),\n",
       " Row(Product_ID=u'P00307242'),\n",
       " Row(Product_ID=u'P00309742'),\n",
       " Row(Product_ID=u'P00365742'),\n",
       " Row(Product_ID=u'P00109442'),\n",
       " Row(Product_ID=u'P00166842'),\n",
       " Row(Product_ID=u'P00200542'),\n",
       " Row(Product_ID=u'P00275342'),\n",
       " Row(Product_ID=u'P00355042'),\n",
       " Row(Product_ID=u'P00056042'),\n",
       " Row(Product_ID=u'P00069842'),\n",
       " Row(Product_ID=u'P00209342'),\n",
       " Row(Product_ID=u'P00357342'),\n",
       " Row(Product_ID=u'P00375436'),\n",
       " Row(Product_ID=u'P00090642'),\n",
       " Row(Product_ID=u'P00208942'),\n",
       " Row(Product_ID=u'P00317242'),\n",
       " Row(Product_ID=u'P00091242'),\n",
       " Row(Product_ID=u'P00141142'),\n",
       " Row(Product_ID=u'P00255042'),\n",
       " Row(Product_ID=u'P00031242'),\n",
       " Row(Product_ID=u'P00013842'),\n",
       " Row(Product_ID=u'P00051742'),\n",
       " Row(Product_ID=u'P00064442'),\n",
       " Row(Product_ID=u'P00104342'),\n",
       " Row(Product_ID=u'P00107342'),\n",
       " Row(Product_ID=u'P00133142'),\n",
       " Row(Product_ID=u'P00143942'),\n",
       " Row(Product_ID=u'P00285142'),\n",
       " Row(Product_ID=u'P00325442'),\n",
       " Row(Product_ID=u'P00366742'),\n",
       " Row(Product_ID=u'P00090042'),\n",
       " Row(Product_ID=u'P00142642'),\n",
       " Row(Product_ID=u'P00157442'),\n",
       " Row(Product_ID=u'P00167342'),\n",
       " Row(Product_ID=u'P00282842'),\n",
       " Row(Product_ID=u'P00044742'),\n",
       " Row(Product_ID=u'P00072242'),\n",
       " Row(Product_ID=u'P00161342'),\n",
       " Row(Product_ID=u'P00334842'),\n",
       " Row(Product_ID=u'P00072142'),\n",
       " Row(Product_ID=u'P00080242'),\n",
       " Row(Product_ID=u'P00013442'),\n",
       " Row(Product_ID=u'P00017842'),\n",
       " Row(Product_ID=u'P00152042'),\n",
       " Row(Product_ID=u'P00267842'),\n",
       " Row(Product_ID=u'P00281242'),\n",
       " Row(Product_ID=u'P00007442'),\n",
       " Row(Product_ID=u'P00217642'),\n",
       " Row(Product_ID=u'P00023442'),\n",
       " Row(Product_ID=u'P00060042'),\n",
       " Row(Product_ID=u'P00075242'),\n",
       " Row(Product_ID=u'P00311842'),\n",
       " Row(Product_ID=u'P00027242'),\n",
       " Row(Product_ID=u'P00037742'),\n",
       " Row(Product_ID=u'P00052342'),\n",
       " Row(Product_ID=u'P00074842'),\n",
       " Row(Product_ID=u'P00078342'),\n",
       " Row(Product_ID=u'P00336542'),\n",
       " Row(Product_ID=u'P00056342'),\n",
       " Row(Product_ID=u'P00124142'),\n",
       " Row(Product_ID=u'P00159642'),\n",
       " Row(Product_ID=u'P00225242'),\n",
       " Row(Product_ID=u'P00367842'),\n",
       " Row(Product_ID=u'P00137742'),\n",
       " Row(Product_ID=u'P00340242'),\n",
       " Row(Product_ID=u'P00136142'),\n",
       " Row(Product_ID=u'P00215042'),\n",
       " Row(Product_ID=u'P00268842'),\n",
       " Row(Product_ID=u'P00159042'),\n",
       " Row(Product_ID=u'P00300742'),\n",
       " Row(Product_ID=u'P00342542'),\n",
       " Row(Product_ID=u'P00069442'),\n",
       " Row(Product_ID=u'P00158942'),\n",
       " Row(Product_ID=u'P00263342'),\n",
       " Row(Product_ID=u'P00275442'),\n",
       " Row(Product_ID=u'P00272742'),\n",
       " Row(Product_ID=u'P00349742'),\n",
       " Row(Product_ID=u'P00016142'),\n",
       " Row(Product_ID=u'P00154142'),\n",
       " Row(Product_ID=u'P00165042'),\n",
       " Row(Product_ID=u'P00205342'),\n",
       " Row(Product_ID=u'P00350842'),\n",
       " Row(Product_ID=u'P00290142'),\n",
       " Row(Product_ID=u'P00343642'),\n",
       " Row(Product_ID=u'P00349542'),\n",
       " Row(Product_ID=u'P00270742'),\n",
       " Row(Product_ID=u'P00319742'),\n",
       " Row(Product_ID=u'P00066042'),\n",
       " Row(Product_ID=u'P00172142'),\n",
       " Row(Product_ID=u'P00193942'),\n",
       " Row(Product_ID=u'P00273142'),\n",
       " Row(Product_ID=u'P00320642'),\n",
       " Row(Product_ID=u'P00361542'),\n",
       " Row(Product_ID=u'P00365442'),\n",
       " Row(Product_ID=u'P0093742'),\n",
       " Row(Product_ID=u'P00033842'),\n",
       " Row(Product_ID=u'P00041742'),\n",
       " Row(Product_ID=u'P00053242'),\n",
       " Row(Product_ID=u'P00138642'),\n",
       " Row(Product_ID=u'P00290842'),\n",
       " Row(Product_ID=u'P00208542'),\n",
       " Row(Product_ID=u'P00173142'),\n",
       " Row(Product_ID=u'P00273042'),\n",
       " Row(Product_ID=u'P00358842'),\n",
       " Row(Product_ID=u'P00203442'),\n",
       " Row(Product_ID=u'P00273842'),\n",
       " Row(Product_ID=u'P00281442'),\n",
       " Row(Product_ID=u'P00307842'),\n",
       " Row(Product_ID=u'P00314742'),\n",
       " Row(Product_ID=u'P00315142'),\n",
       " Row(Product_ID=u'P00009642'),\n",
       " Row(Product_ID=u'P00014142'),\n",
       " Row(Product_ID=u'P00171242'),\n",
       " Row(Product_ID=u'P00202642'),\n",
       " Row(Product_ID=u'P00249442'),\n",
       " Row(Product_ID=u'P00342242'),\n",
       " Row(Product_ID=u'P00348142'),\n",
       " Row(Product_ID=u'P00054842'),\n",
       " Row(Product_ID=u'P00143242'),\n",
       " Row(Product_ID=u'P00157242'),\n",
       " Row(Product_ID=u'P00174642'),\n",
       " Row(Product_ID=u'P00232942'),\n",
       " Row(Product_ID=u'P00239342'),\n",
       " Row(Product_ID=u'P00364542'),\n",
       " Row(Product_ID=u'P00013242'),\n",
       " Row(Product_ID=u'P00050042'),\n",
       " Row(Product_ID=u'P00132542'),\n",
       " Row(Product_ID=u'P00216842'),\n",
       " Row(Product_ID=u'P00262642'),\n",
       " Row(Product_ID=u'P00339742'),\n",
       " Row(Product_ID=u'P00081342'),\n",
       " Row(Product_ID=u'P00135942'),\n",
       " Row(Product_ID=u'P00310442'),\n",
       " Row(Product_ID=u'P00036042'),\n",
       " Row(Product_ID=u'P00109342'),\n",
       " Row(Product_ID=u'P00200242'),\n",
       " Row(Product_ID=u'P00009742'),\n",
       " Row(Product_ID=u'P00039842'),\n",
       " Row(Product_ID=u'P00072042'),\n",
       " Row(Product_ID=u'P00290442'),\n",
       " Row(Product_ID=u'P00309542'),\n",
       " Row(Product_ID=u'P00121942'),\n",
       " Row(Product_ID=u'P00123942'),\n",
       " Row(Product_ID=u'P00125242'),\n",
       " Row(Product_ID=u'P00316142'),\n",
       " Row(Product_ID=u'P00318442'),\n",
       " Row(Product_ID=u'P00329042'),\n",
       " Row(Product_ID=u'P00362142'),\n",
       " Row(Product_ID=u'P0097442'),\n",
       " Row(Product_ID=u'P00079342'),\n",
       " Row(Product_ID=u'P00168942'),\n",
       " Row(Product_ID=u'P00014442'),\n",
       " Row(Product_ID=u'P00017742'),\n",
       " Row(Product_ID=u'P00200742'),\n",
       " Row(Product_ID=u'P00286942'),\n",
       " Row(Product_ID=u'P00288142'),\n",
       " Row(Product_ID=u'P00012342'),\n",
       " Row(Product_ID=u'P00012442'),\n",
       " Row(Product_ID=u'P00029342'),\n",
       " Row(Product_ID=u'P00035042'),\n",
       " Row(Product_ID=u'P00080542'),\n",
       " Row(Product_ID=u'P00164442'),\n",
       " Row(Product_ID=u'P00205742'),\n",
       " Row(Product_ID=u'P00267042'),\n",
       " Row(Product_ID=u'P00082042'),\n",
       " Row(Product_ID=u'P00104242'),\n",
       " Row(Product_ID=u'P00252942'),\n",
       " Row(Product_ID=u'P00315242'),\n",
       " Row(Product_ID=u'P00363342'),\n",
       " Row(Product_ID=u'P00003342'),\n",
       " Row(Product_ID=u'P00005242'),\n",
       " Row(Product_ID=u'P00018442'),\n",
       " Row(Product_ID=u'P00280042'),\n",
       " Row(Product_ID=u'P00292242'),\n",
       " Row(Product_ID=u'P00337042'),\n",
       " Row(Product_ID=u'P0093842'),\n",
       " Row(Product_ID=u'P00071342'),\n",
       " Row(Product_ID=u'P00083542'),\n",
       " Row(Product_ID=u'P00251142'),\n",
       " Row(Product_ID=u'P00258042'),\n",
       " Row(Product_ID=u'P00059342'),\n",
       " Row(Product_ID=u'P00063442'),\n",
       " Row(Product_ID=u'P00130042'),\n",
       " Row(Product_ID=u'P00169942'),\n",
       " Row(Product_ID=u'P00215142'),\n",
       " Row(Product_ID=u'P0096242'),\n",
       " Row(Product_ID=u'P00027842'),\n",
       " Row(Product_ID=u'P00029042'),\n",
       " Row(Product_ID=u'P00045942'),\n",
       " Row(Product_ID=u'P00065142'),\n",
       " Row(Product_ID=u'P00136042'),\n",
       " Row(Product_ID=u'P00213442'),\n",
       " Row(Product_ID=u'P00230142'),\n",
       " Row(Product_ID=u'P00237042'),\n",
       " Row(Product_ID=u'P00105842'),\n",
       " Row(Product_ID=u'P00167542'),\n",
       " Row(Product_ID=u'P00276242'),\n",
       " Row(Product_ID=u'P00308942'),\n",
       " Row(Product_ID=u'P00342042'),\n",
       " Row(Product_ID=u'P00360342'),\n",
       " Row(Product_ID=u'P00064342'),\n",
       " Row(Product_ID=u'P00103542'),\n",
       " Row(Product_ID=u'P00104642'),\n",
       " Row(Product_ID=u'P00146642'),\n",
       " Row(Product_ID=u'P00305542'),\n",
       " Row(Product_ID=u'P00341042'),\n",
       " Row(Product_ID=u'P00068642'),\n",
       " Row(Product_ID=u'P00205542'),\n",
       " Row(Product_ID=u'P00248642'),\n",
       " Row(Product_ID=u'P00334742'),\n",
       " Row(Product_ID=u'P00339142'),\n",
       " Row(Product_ID=u'P00352042'),\n",
       " Row(Product_ID=u'P00206242'),\n",
       " Row(Product_ID=u'P00248242'),\n",
       " Row(Product_ID=u'P00299942'),\n",
       " Row(Product_ID=u'P00197242'),\n",
       " Row(Product_ID=u'P00272042'),\n",
       " Row(Product_ID=u'P00272342'),\n",
       " Row(Product_ID=u'P00325342'),\n",
       " Row(Product_ID=u'P00335842'),\n",
       " Row(Product_ID=u'P00361742'),\n",
       " Row(Product_ID=u'P00016242'),\n",
       " Row(Product_ID=u'P00203842'),\n",
       " Row(Product_ID=u'P00280242'),\n",
       " Row(Product_ID=u'P00312042'),\n",
       " Row(Product_ID=u'P00292542'),\n",
       " Row(Product_ID=u'P00297142'),\n",
       " Row(Product_ID=u'P00314942'),\n",
       " Row(Product_ID=u'P00353542'),\n",
       " Row(Product_ID=u'P00009442'),\n",
       " Row(Product_ID=u'P00232242'),\n",
       " Row(Product_ID=u'P00247842'),\n",
       " Row(Product_ID=u'P00333442'),\n",
       " Row(Product_ID=u'P00360442'),\n",
       " Row(Product_ID=u'P00011242'),\n",
       " Row(Product_ID=u'P00039142'),\n",
       " Row(Product_ID=u'P00133942'),\n",
       " Row(Product_ID=u'P00175142'),\n",
       " Row(Product_ID=u'P00324142'),\n",
       " Row(Product_ID=u'P00364342'),\n",
       " Row(Product_ID=u'P00370853'),\n",
       " Row(Product_ID=u'P00061542'),\n",
       " Row(Product_ID=u'P00149742'),\n",
       " Row(Product_ID=u'P00040542'),\n",
       " Row(Product_ID=u'P00234242'),\n",
       " Row(Product_ID=u'P00308842'),\n",
       " Row(Product_ID=u'P00330742'),\n",
       " Row(Product_ID=u'P00122042'),\n",
       " Row(Product_ID=u'P00152842'),\n",
       " Row(Product_ID=u'P00201042'),\n",
       " Row(Product_ID=u'P00235742'),\n",
       " Row(Product_ID=u'P00253342'),\n",
       " Row(Product_ID=u'P00008242'),\n",
       " Row(Product_ID=u'P00017442'),\n",
       " Row(Product_ID=u'P00208642'),\n",
       " Row(Product_ID=u'P00262142'),\n",
       " Row(Product_ID=u'P00299042'),\n",
       " Row(Product_ID=u'P00090142'),\n",
       " Row(Product_ID=u'P00104042'),\n",
       " Row(Product_ID=u'P00262742'),\n",
       " Row(Product_ID=u'P00318042'),\n",
       " Row(Product_ID=u'P00328242'),\n",
       " Row(Product_ID=u'P00011542'),\n",
       " Row(Product_ID=u'P00306342'),\n",
       " Row(Product_ID=u'P00308042'),\n",
       " Row(Product_ID=u'P00051242'),\n",
       " Row(Product_ID=u'P00108742'),\n",
       " Row(Product_ID=u'P00235942'),\n",
       " Row(Product_ID=u'P00286242'),\n",
       " Row(Product_ID=u'P00299742'),\n",
       " Row(Product_ID=u'P00347742'),\n",
       " Row(Product_ID=u'P00067042'),\n",
       " Row(Product_ID=u'P00073342'),\n",
       " Row(Product_ID=u'P00163642'),\n",
       " Row(Product_ID=u'P00202842'),\n",
       " Row(Product_ID=u'P00055242'),\n",
       " Row(Product_ID=u'P00211442'),\n",
       " Row(Product_ID=u'P00238842'),\n",
       " Row(Product_ID=u'P00239442'),\n",
       " Row(Product_ID=u'P00269042'),\n",
       " Row(Product_ID=u'P00192342'),\n",
       " Row(Product_ID=u'P00236442'),\n",
       " Row(Product_ID=u'P00363542'),\n",
       " Row(Product_ID=u'P00194942'),\n",
       " Row(Product_ID=u'P00301942'),\n",
       " Row(Product_ID=u'P00348042'),\n",
       " Row(Product_ID=u'P00188342'),\n",
       " Row(Product_ID=u'P00220542'),\n",
       " Row(Product_ID=u'P00284342'),\n",
       " Row(Product_ID=u'P00294342'),\n",
       " Row(Product_ID=u'P00305342'),\n",
       " Row(Product_ID=u'P00012042'),\n",
       " Row(Product_ID=u'P00203042'),\n",
       " Row(Product_ID=u'P00264442'),\n",
       " Row(Product_ID=u'P00330042'),\n",
       " Row(Product_ID=u'P00038042'),\n",
       " Row(Product_ID=u'P00040942'),\n",
       " Row(Product_ID=u'P00048842'),\n",
       " Row(Product_ID=u'P00060542'),\n",
       " Row(Product_ID=u'P00301442'),\n",
       " Row(Product_ID=u'P00312542'),\n",
       " Row(Product_ID=u'P00164242'),\n",
       " Row(Product_ID=u'P00290742'),\n",
       " Row(Product_ID=u'P00018742'),\n",
       " Row(Product_ID=u'P00211742'),\n",
       " Row(Product_ID=u'P00301042'),\n",
       " Row(Product_ID=u'P00306742'),\n",
       " Row(Product_ID=u'P00324042'),\n",
       " Row(Product_ID=u'P00337142'),\n",
       " Row(Product_ID=u'P00356142'),\n",
       " Row(Product_ID=u'P00069942'),\n",
       " Row(Product_ID=u'P00292142'),\n",
       " Row(Product_ID=u'P00070742'),\n",
       " Row(Product_ID=u'P00071642'),\n",
       " Row(Product_ID=u'P00073242'),\n",
       " Row(Product_ID=u'P00108342'),\n",
       " Row(Product_ID=u'P00181042'),\n",
       " Row(Product_ID=u'P00186142'),\n",
       " Row(Product_ID=u'P00229142'),\n",
       " Row(Product_ID=u'P00247242'),\n",
       " Row(Product_ID=u'P00275042'),\n",
       " Row(Product_ID=u'P0098042'),\n",
       " Row(Product_ID=u'P00007642'),\n",
       " Row(Product_ID=u'P00068742'),\n",
       " Row(Product_ID=u'P00091942'),\n",
       " Row(Product_ID=u'P00144142'),\n",
       " Row(Product_ID=u'P00153942'),\n",
       " Row(Product_ID=u'P00219642'),\n",
       " Row(Product_ID=u'P00006642'),\n",
       " Row(Product_ID=u'P00017942'),\n",
       " Row(Product_ID=u'P00080142'),\n",
       " Row(Product_ID=u'P00144442'),\n",
       " Row(Product_ID=u'P00256342'),\n",
       " Row(Product_ID=u'P00335342'),\n",
       " Row(Product_ID=u'P00362542'),\n",
       " Row(Product_ID=u'P00008942'),\n",
       " Row(Product_ID=u'P00242642'),\n",
       " Row(Product_ID=u'P00264942'),\n",
       " Row(Product_ID=u'P00353342'),\n",
       " Row(Product_ID=u'P00353642'),\n",
       " Row(Product_ID=u'P00023842'),\n",
       " Row(Product_ID=u'P00054142'),\n",
       " Row(Product_ID=u'P00201142'),\n",
       " Row(Product_ID=u'P00241742'),\n",
       " Row(Product_ID=u'P00301642'),\n",
       " Row(Product_ID=u'P00104442'),\n",
       " Row(Product_ID=u'P00279742'),\n",
       " Row(Product_ID=u'P00353442'),\n",
       " Row(Product_ID=u'P0097642'),\n",
       " Row(Product_ID=u'P00049042'),\n",
       " Row(Product_ID=u'P00055342'),\n",
       " Row(Product_ID=u'P00073442'),\n",
       " Row(Product_ID=u'P00074742'),\n",
       " Row(Product_ID=u'P00156342'),\n",
       " Row(Product_ID=u'P00231042'),\n",
       " Row(Product_ID=u'P00122942'),\n",
       " Row(Product_ID=u'P00140242'),\n",
       " Row(Product_ID=u'P00295542'),\n",
       " Row(Product_ID=u'P00026842'),\n",
       " Row(Product_ID=u'P00055142'),\n",
       " Row(Product_ID=u'P00106342'),\n",
       " Row(Product_ID=u'P00143842'),\n",
       " Row(Product_ID=u'P00224642'),\n",
       " Row(Product_ID=u'P00240742'),\n",
       " Row(Product_ID=u'P00272642'),\n",
       " Row(Product_ID=u'P00308442'),\n",
       " Row(Product_ID=u'P00356442'),\n",
       " Row(Product_ID=u'P00357542'),\n",
       " Row(Product_ID=u'P00072442'),\n",
       " Row(Product_ID=u'P00093442'),\n",
       " Row(Product_ID=u'P00202442'),\n",
       " Row(Product_ID=u'P00223442'),\n",
       " Row(Product_ID=u'P00234942'),\n",
       " Row(Product_ID=u'P00295642'),\n",
       " Row(Product_ID=u'P00089642'),\n",
       " Row(Product_ID=u'P00369942'),\n",
       " Row(Product_ID=u'P00081542'),\n",
       " Row(Product_ID=u'P00090442'),\n",
       " Row(Product_ID=u'P00131942'),\n",
       " Row(Product_ID=u'P00140142'),\n",
       " Row(Product_ID=u'P00148142'),\n",
       " Row(Product_ID=u'P00238242'),\n",
       " Row(Product_ID=u'P00257142'),\n",
       " Row(Product_ID=u'P00263742'),\n",
       " Row(Product_ID=u'P00296742'),\n",
       " Row(Product_ID=u'P00019642'),\n",
       " Row(Product_ID=u'P00005342'),\n",
       " Row(Product_ID=u'P00014342'),\n",
       " Row(Product_ID=u'P00142542'),\n",
       " Row(Product_ID=u'P00299342'),\n",
       " Row(Product_ID=u'P00302242'),\n",
       " Row(Product_ID=u'P00358542'),\n",
       " Row(Product_ID=u'P00013342'),\n",
       " Row(Product_ID=u'P00106142'),\n",
       " Row(Product_ID=u'P00159142'),\n",
       " Row(Product_ID=u'P00325042'),\n",
       " Row(Product_ID=u'P00366042'),\n",
       " Row(Product_ID=u'P00102742'),\n",
       " Row(Product_ID=u'P00268642'),\n",
       " Row(Product_ID=u'P00336442'),\n",
       " Row(Product_ID=u'P00355442'),\n",
       " Row(Product_ID=u'P00057842'),\n",
       " Row(Product_ID=u'P00123242'),\n",
       " Row(Product_ID=u'P00141642'),\n",
       " Row(Product_ID=u'P00164342'),\n",
       " Row(Product_ID=u'P00162842'),\n",
       " Row(Product_ID=u'P00225342'),\n",
       " Row(Product_ID=u'P00241942'),\n",
       " Row(Product_ID=u'P00299842'),\n",
       " Row(Product_ID=u'P00333842'),\n",
       " Row(Product_ID=u'P00336342'),\n",
       " Row(Product_ID=u'P00146542'),\n",
       " Row(Product_ID=u'P00309042'),\n",
       " Row(Product_ID=u'P00334542'),\n",
       " Row(Product_ID=u'P00369442'),\n",
       " Row(Product_ID=u'P00038742'),\n",
       " Row(Product_ID=u'P00060442'),\n",
       " Row(Product_ID=u'P00069342'),\n",
       " Row(Product_ID=u'P00015142'),\n",
       " Row(Product_ID=u'P00026242'),\n",
       " Row(Product_ID=u'P00086742'),\n",
       " Row(Product_ID=u'P00203242'),\n",
       " Row(Product_ID=u'P00210842'),\n",
       " Row(Product_ID=u'P00359942'),\n",
       " Row(Product_ID=u'P00245242'),\n",
       " Row(Product_ID=u'P00268142'),\n",
       " Row(Product_ID=u'P00158442'),\n",
       " Row(Product_ID=u'P00169542'),\n",
       " Row(Product_ID=u'P00364742'),\n",
       " Row(Product_ID=u'P00091542'),\n",
       " Row(Product_ID=u'P00306542'),\n",
       " Row(Product_ID=u'P00051342'),\n",
       " Row(Product_ID=u'P00243542'),\n",
       " Row(Product_ID=u'P00266342'),\n",
       " Row(Product_ID=u'P00300442'),\n",
       " Row(Product_ID=u'P00071742'),\n",
       " Row(Product_ID=u'P00134442'),\n",
       " Row(Product_ID=u'P00245842'),\n",
       " Row(Product_ID=u'P00256442'),\n",
       " Row(Product_ID=u'P00308642'),\n",
       " Row(Product_ID=u'P00314042'),\n",
       " Row(Product_ID=u'P00362942'),\n",
       " Row(Product_ID=u'P00081842'),\n",
       " Row(Product_ID=u'P00154342'),\n",
       " Row(Product_ID=u'P00301142'),\n",
       " Row(Product_ID=u'P00341442'),\n",
       " Row(Product_ID=u'P00342942'),\n",
       " Row(Product_ID=u'P00368742'),\n",
       " Row(Product_ID=u'P0097542'),\n",
       " Row(Product_ID=u'P00073542'),\n",
       " Row(Product_ID=u'P00243142'),\n",
       " Row(Product_ID=u'P00273642'),\n",
       " Row(Product_ID=u'P00293442'),\n",
       " Row(Product_ID=u'P00335642'),\n",
       " Row(Product_ID=u'P00032742'),\n",
       " Row(Product_ID=u'P00355742'),\n",
       " Row(Product_ID=u'P00247442'),\n",
       " Row(Product_ID=u'P00253942'),\n",
       " Row(Product_ID=u'P00276042'),\n",
       " Row(Product_ID=u'P00038442'),\n",
       " Row(Product_ID=u'P00132942'),\n",
       " Row(Product_ID=u'P00162142'),\n",
       " Row(Product_ID=u'P00008342'),\n",
       " Row(Product_ID=u'P00028742'),\n",
       " Row(Product_ID=u'P00055942'),\n",
       " Row(Product_ID=u'P00355842'),\n",
       " Row(Product_ID=u'P00000842'),\n",
       " Row(Product_ID=u'P00007742'),\n",
       " Row(Product_ID=u'P00007942'),\n",
       " Row(Product_ID=u'P00062442'),\n",
       " Row(Product_ID=u'P00092042'),\n",
       " Row(Product_ID=u'P00200942'),\n",
       " Row(Product_ID=u'P00240342'),\n",
       " Row(Product_ID=u'P00246842'),\n",
       " Row(Product_ID=u'P00261042'),\n",
       " Row(Product_ID=u'P00311942'),\n",
       " Row(Product_ID=u'P00316342'),\n",
       " Row(Product_ID=u'P00015042'),\n",
       " Row(Product_ID=u'P00055042'),\n",
       " Row(Product_ID=u'P00067342'),\n",
       " Row(Product_ID=u'P00364942'),\n",
       " Row(Product_ID=u'P00081642'),\n",
       " Row(Product_ID=u'P00100142'),\n",
       " Row(Product_ID=u'P00159342'),\n",
       " Row(Product_ID=u'P00175642'),\n",
       " Row(Product_ID=u'P00307042'),\n",
       " Row(Product_ID=u'P00024442'),\n",
       " Row(Product_ID=u'P00161742'),\n",
       " Row(Product_ID=u'P00201842'),\n",
       " Row(Product_ID=u'P00238942'),\n",
       " Row(Product_ID=u'P00320142'),\n",
       " Row(Product_ID=u'P00149242'),\n",
       " Row(Product_ID=u'P00231642'),\n",
       " Row(Product_ID=u'P00345042'),\n",
       " Row(Product_ID=u'P00311442'),\n",
       " Row(Product_ID=u'P00140542'),\n",
       " Row(Product_ID=u'P00228842'),\n",
       " Row(Product_ID=u'P00241842'),\n",
       " Row(Product_ID=u'P00261442'),\n",
       " Row(Product_ID=u'P00277342'),\n",
       " Row(Product_ID=u'P00332742'),\n",
       " Row(Product_ID=u'P00337242'),\n",
       " Row(Product_ID=u'P00058542'),\n",
       " Row(Product_ID=u'P00069642'),\n",
       " Row(Product_ID=u'P00172242'),\n",
       " Row(Product_ID=u'P00329942'),\n",
       " Row(Product_ID=u'P00012942'),\n",
       " Row(Product_ID=u'P00103442'),\n",
       " Row(Product_ID=u'P00199242'),\n",
       " Row(Product_ID=u'P00279242'),\n",
       " Row(Product_ID=u'P00287042'),\n",
       " Row(Product_ID=u'P00023342'),\n",
       " Row(Product_ID=u'P00169342'),\n",
       " Row(Product_ID=u'P00317742'),\n",
       " Row(Product_ID=u'P00347042'),\n",
       " Row(Product_ID=u'P00009142'),\n",
       " Row(Product_ID=u'P00103942'),\n",
       " Row(Product_ID=u'P00261642'),\n",
       " Row(Product_ID=u'P00318542'),\n",
       " Row(Product_ID=u'P00362242'),\n",
       " Row(Product_ID=u'P00074542'),\n",
       " Row(Product_ID=u'P00075142'),\n",
       " Row(Product_ID=u'P00203742'),\n",
       " Row(Product_ID=u'P00261742'),\n",
       " Row(Product_ID=u'P00274742'),\n",
       " Row(Product_ID=u'P00047142'),\n",
       " Row(Product_ID=u'P00077142'),\n",
       " Row(Product_ID=u'P00077342'),\n",
       " Row(Product_ID=u'P00243042'),\n",
       " Row(Product_ID=u'P00261942'),\n",
       " Row(Product_ID=u'P00341142'),\n",
       " Row(Product_ID=u'P00133042'),\n",
       " Row(Product_ID=u'P00144942'),\n",
       " Row(Product_ID=u'P00157842'),\n",
       " Row(Product_ID=u'P00238042'),\n",
       " Row(Product_ID=u'P00311242'),\n",
       " Row(Product_ID=u'P00330542'),\n",
       " Row(Product_ID=u'P00021242'),\n",
       " Row(Product_ID=u'P00298342'),\n",
       " Row(Product_ID=u'P00308142'),\n",
       " Row(Product_ID=u'P00333942'),\n",
       " Row(Product_ID=u'P00038842'),\n",
       " Row(Product_ID=u'P00159242'),\n",
       " Row(Product_ID=u'P00262942'),\n",
       " Row(Product_ID=u'P00340142'),\n",
       " Row(Product_ID=u'P00364242'),\n",
       " Row(Product_ID=u'P00370293'),\n",
       " Row(Product_ID=u'P00005842'),\n",
       " Row(Product_ID=u'P00011342'),\n",
       " Row(Product_ID=u'P00123542'),\n",
       " Row(Product_ID=u'P00168542'),\n",
       " Row(Product_ID=u'P00224242'),\n",
       " Row(Product_ID=u'P00278742'),\n",
       " Row(Product_ID=u'P00324242'),\n",
       " Row(Product_ID=u'P00063842'),\n",
       " Row(Product_ID=u'P00107442'),\n",
       " Row(Product_ID=u'P00146942'),\n",
       " Row(Product_ID=u'P00186742'),\n",
       " Row(Product_ID=u'P00299642'),\n",
       " Row(Product_ID=u'P00351742'),\n",
       " Row(Product_ID=u'P00131842'),\n",
       " Row(Product_ID=u'P00004942'),\n",
       " Row(Product_ID=u'P00047542'),\n",
       " Row(Product_ID=u'P00076742'),\n",
       " Row(Product_ID=u'P00114842'),\n",
       " Row(Product_ID=u'P00323342'),\n",
       " Row(Product_ID=u'P00229042'),\n",
       " Row(Product_ID=u'P00175842'),\n",
       " Row(Product_ID=u'P00185942'),\n",
       " Row(Product_ID=u'P00326342'),\n",
       " Row(Product_ID=u'P00149842'),\n",
       " Row(Product_ID=u'P00256242'),\n",
       " Row(Product_ID=u'P00326042'),\n",
       " Row(Product_ID=u'P00353742'),\n",
       " Row(Product_ID=u'P00367342'),\n",
       " Row(Product_ID=u'P00069742'),\n",
       " Row(Product_ID=u'P00076242'),\n",
       " Row(Product_ID=u'P00107742'),\n",
       " Row(Product_ID=u'P00155842'),\n",
       " Row(Product_ID=u'P00229542'),\n",
       " Row(Product_ID=u'P00019742'),\n",
       " Row(Product_ID=u'P00031542'),\n",
       " Row(Product_ID=u'P00141742'),\n",
       " Row(Product_ID=u'P00154942'),\n",
       " Row(Product_ID=u'P00158142'),\n",
       " Row(Product_ID=u'P00305942'),\n",
       " Row(Product_ID=u'P00321242'),\n",
       " Row(Product_ID=u'P00332642'),\n",
       " Row(Product_ID=u'P00348442'),\n",
       " Row(Product_ID=u'P00349042'),\n",
       " Row(Product_ID=u'P00054642'),\n",
       " Row(Product_ID=u'P00133342'),\n",
       " Row(Product_ID=u'P00257242'),\n",
       " Row(Product_ID=u'P00360142')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The output 1 means we have now only 1 different category k and train.\n",
    "diff_cat_in_train_test.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, round, bround\n",
    "trainDF.select(round(lit(\"2.5\")), bround(lit(2.5))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|            3|             2|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT round(2.5), bround(2.5)\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.34370334592\n",
      "+----------------------------------+\n",
      "|corr(Purchase, Product_Category_1)|\n",
      "+----------------------------------+\n",
      "|              -0.34370334591990637|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "print(trainDF.stat.corr(\"Purchase\", \"Product_Category_1\"))\n",
    "trainDF.select(corr(\"Purchase\", \"Product_Category_1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|corr(CAST(Purchase AS DOUBLE), CAST(Product_Category_1 AS DOUBLE))|\n",
      "+------------------------------------------------------------------+\n",
      "|                                              -0.34370334591990637|\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT corr(Purchase, Product_Category_1) FROM trainDFTable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+----------+----------+\n",
      "| ltrim| rtrim| trim|        lp|        rp|\n",
      "+------+------+-----+----------+----------+\n",
      "|HELLO | HELLO|HELLO|11111HELLO|HELLO11111|\n",
      "|HELLO | HELLO|HELLO|11111HELLO|HELLO11111|\n",
      "+------+------+-----+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "\n",
    "trainDF.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 10, \"1\").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \"1\").alias(\"rp\"))\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "| Gender_DECODE|Gender|\n",
      "+--------------+------+\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "+--------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"F|M\"\n",
    "\n",
    "trainDF.select(\n",
    "regexp_replace(col(\"Gender\"), regex_string, \"MALE_OR_FEMALE\")\n",
    ".alias(\"Gender_DECODE\"),\n",
    "col(\"Gender\"))\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "| Gender_DECODE|Gender|\n",
      "+--------------+------+\n",
      "|MALE_OR_FEMALE|     F|\n",
      "|MALE_OR_FEMALE|     F|\n",
      "+--------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "regexp_replace(Gender, 'F|M', 'MALE_OR_FEMALE') as\n",
    "Gender_DECODE,\n",
    "Gender\n",
    "FROM\n",
    "trainDFTable\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Date and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2019-02-15|2019-02-15 18:06:14.962|\n",
      "|1  |2019-02-15|2019-02-15 18:06:14.962|\n",
      "|2  |2019-02-15|2019-02-15 18:06:14.962|\n",
      "|3  |2019-02-15|2019-02-15 18:06:14.962|\n",
      "|4  |2019-02-15|2019-02-15 18:06:14.962|\n",
      "|5  |2019-02-15|2019-02-15 18:06:14.962|\n",
      "|6  |2019-02-15|2019-02-15 18:06:14.962|\n",
      "|7  |2019-02-15|2019-02-15 18:06:14.962|\n",
      "|8  |2019-02-15|2019-02-15 18:06:14.962|\n",
      "|9  |2019-02-15|2019-02-15 18:06:14.962|\n",
      "+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "dateDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.createOrReplaceTempView(\"dateDFTable\")\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2019-02-10|        2019-02-20|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.select(date_sub(col(\"today\"), 5),date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2019-02-10|        2019-02-20|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "date_sub(today, 5),\n",
    "date_add(today, 5)\n",
    "FROM\n",
    "dateDFTable\n",
    "\"\"\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(today, week_ago)|\n",
      "+-------------------------+\n",
      "|                        7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "dateDF\\\n",
    ".withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    ".select(datediff(col(\"today\"), col(\"week_ago\")))\\\n",
    ".show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|months_between(end, start)|\n",
      "+--------------------------+\n",
      "|                13.5483871|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF\\\n",
    ".select(\n",
    "to_date(lit(\"2017-01-01\")).alias(\"start\"),\n",
    "to_date(lit(\"2018-02-18\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"end\"), col(\"start\")))\\\n",
    ".show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------------------------------------------------------------+------------------------------------------------------------+\n",
      "|to_date('2016-01-01')|months_between(CAST(2016-01-01 AS TIMESTAMP), CAST(2017-01-01 AS TIMESTAMP))|datediff(CAST(2016-01-01 AS DATE), CAST(2017-01-01 AS DATE))|\n",
      "+---------------------+----------------------------------------------------------------------------+------------------------------------------------------------+\n",
      "|           2016-01-01|                                                                       -12.0|                                                        -366|\n",
      "|           2016-01-01|                                                                       -12.0|                                                        -366|\n",
      "+---------------------+----------------------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "to_date('2016-01-01'),\n",
    "months_between('2016-01-01', '2017-01-01'),\n",
    "datediff('2016-01-01', '2017-01-01')\n",
    "FROM\n",
    "dateDFTable\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n",
    ".select(to_date(col(\"date\")))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WARNING__\n",
    "<br>Spark will not throw an error if it cannot parse the date, it’ll just return null. This can be a bit tricky in larger pipelines because you may be expecting your data in one format and getting it in another. To illustrate, let’s take a look at the date format that has switched from year-month-day to year-day-month. Spark will fail to parse this date and silently return null instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 2016-20-12 - year-day-month\n",
    "### 2017-12-11 - year-month-day\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "\n",
    "cleanDateDF = spark.range(1)\\\n",
    ".select(to_date(unix_timestamp(lit(\"2017-12-11\"), dateFormat)\n",
    ".cast(\"timestamp\"))\\\n",
    ".alias(\"date\"),\n",
    "to_date(unix_timestamp(lit(\"2017-20-12\"), dateFormat)\n",
    ".cast(\"timestamp\"))\\\n",
    ".alias(\"date2\"))\n",
    "\n",
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------+\n",
      "|to_date(CAST(unix_timestamp(datetable2.`date`, 'yyyy-dd-MM') AS TIMESTAMP))|to_date(CAST(unix_timestamp(datetable2.`date2`, 'yyyy-dd-MM') AS TIMESTAMP))|to_date(datetable2.`date`)|\n",
      "+---------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------+\n",
      "|                                                                 2017-11-12|                                                                  2017-12-20|                2017-11-12|\n",
      "+---------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "to_date(cast(unix_timestamp(date, 'yyyy-dd-MM') as timestamp)),\n",
    "to_date(cast(unix_timestamp(date2, 'yyyy-dd-MM') as timestamp)),\n",
    "to_date(date)\n",
    "FROM\n",
    "dateTable2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|CAST(unix_timestamp(date, yyyy-dd-MM) AS TIMESTAMP)|\n",
      "+---------------------------------------------------+\n",
      "|2017-11-12 00:00:00                                |\n",
      "+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF\\\n",
    ".select(unix_timestamp(col(\"date\"), dateFormat).cast(\"timestamp\"))\\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the function is created, we need to register them with Spark so that we can use\n",
    "them on all of our worker machines. Spark will serialize the function on the driver, and transfer it over the network to all executor processes. This happens regardless of language.\n",
    "\n",
    "<br>Once we go to use the function, there are essentially two different things that occur. If the function is written in Scala or Java then we can use that function within the JVM. This means there will be little performance penalty aside from the fact that we can’t take advantage of code generation capabilities that Spark has for built-in functions.\n",
    "\n",
    "<br>If the function is written in Python, something quite different happens. \n",
    "Spark will start up a python process on the worker, serialize all of the data to a format that python can understand (remember it was in the JVM before), execute the function row by row on that data in the python process, before finally returning the results of the row operations to the JVM and Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=udfExampleDF.select(power3(col(\"num\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|POWER(num, 3)|\n",
      "+-------------+\n",
      "|          0.0|\n",
      "|          1.0|\n",
      "|          8.0|\n",
      "|         27.0|\n",
      "|         64.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Different Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are variety of data sources that one can use out of the box aswell as the countless other sources built by the greater community.\n",
    "\n",
    "<br> **Spark** has six “core” data sources and hundreds of external data sources written by the community.\n",
    "\n",
    "-  CSV\n",
    "-  JSON\n",
    "-  Parquet\n",
    "-  ORC\n",
    "-  JDBC/ODBC Connections\n",
    "-  Plain-text files\n",
    "\n",
    "<br> As mentioned, Spark has numerous community-created data sources. Here’s just a small sample:\n",
    "-  Cassandra\n",
    "-  HBase\n",
    "-  MongoDB\n",
    "-  AWS Redshift\n",
    "-  XML\n",
    "-  And many many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read API Structure**\n",
    "<br>DataFrameReader.format(...).option(\"key\", \"value\").schema(...).load(...)\n",
    "<br>After we have a DataFrame reader, we specify several values:\n",
    "-  The format\n",
    "-  The schema\n",
    "-  The read mode\n",
    "-  A series of options\n",
    "\n",
    "*Ex. spark.read.format(\"csv\")\n",
    "<br>  .option(\"mode\", \"FAILFAST\")\n",
    "<br>  .option(\"inferSchema\", \"true\")\n",
    "<br>  .option(\"path\", \"path/to/file(s)\")\n",
    "<br>  .schema(someSchema)\n",
    "<br>  .load()\n",
    "*\n",
    "\n",
    "** READ MODES **\n",
    "-  permissive - Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called _corrupt_record.\n",
    "-  dropMalformed - Drops the row that contains malformed records\n",
    "-  failFast - Fails immediately upon encountering malformed records\n",
    "<br><br>The default is permissive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Write API Structure **\n",
    "<br>We will use this format to write to all of our data sources. \n",
    "<br>format is optional because by default, Spark will use the **Parquet** format. \n",
    "<br>option, again, allows us to configure how to write out our given data. \n",
    "<br>PartitionBy, bucketBy, and sortBy work only for file-based data sources; \n",
    "<br>you can use them to control the specific layout of files at the destination.\n",
    "\n",
    "<br> DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()\n",
    "<br> The foundation for writing data is quite similar to that of reading data. \n",
    "<br>Instead of the DataFrameReader, we have the DataFrameWriter. \n",
    "<br>Because we always need to write out some given data source, \n",
    "<br>we access the DataFrameWriter on a per-DataFrame basis via the write attribute:\n",
    "\n",
    "<br>After we have a DataFrameWriter, we specify three values: the format, a series of options, and the save mode. \n",
    "\n",
    "<br>Example: \n",
    "<br>dataframe.write.format(\"csv\")\n",
    "<br>  .option(\"mode\", \"OVERWRITE\")\n",
    "<br>  .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "<br>  .option(\"path\", \"path/to/file(s)\")\n",
    "<br>  .save()\n",
    "\n",
    "** SAVE MODES **\n",
    "-  append - Appends the output files to the list of files that already exist at that location\n",
    "-  overwrite - Will completely overwrite any data that already exists there\n",
    "-  errorIfExists - Throws an error and fails the write if data or files already exist at the specified location\n",
    "-  ignore - If data or files exist at the location, do nothing with the current DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfsroot/data/home/pavanw/B52/SparkSQL_DF\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 27140\r\n",
      "-rw-rw-r-- 1 pavanw pavanw    14357 Feb 14 15:33 SparkSQL_datasets.ipynb\r\n",
      "-rw-r--r-- 1 pavanw pavanw   265945 Feb 14 15:33 TCS_NSE.csv\r\n",
      "-rw-r--r-- 1 pavanw pavanw   140043 Feb 14 15:33 temp_data.txt\r\n",
      "-rw-r--r-- 1 pavanw pavanw   937409 Feb 14 15:33 test_sample10.csv\r\n",
      "-rw-r--r-- 1 pavanw pavanw 26062111 Feb 14 15:33 train_sample10.csv\r\n",
      "drwxrwxr-x 2 pavanw pavanw       10 Feb 14 17:18 spark-warehouse\r\n",
      "-rw-rw-r-- 1 pavanw pavanw     9789 Feb 14 18:17 Diff_between_global&TempView.ipynb\r\n",
      "-rw-rw-r-- 1 pavanw pavanw   181610 Feb 15 17:55 Spark_SQL&DF.ipynb\r\n",
      "-rw-rw-r-- 1 pavanw pavanw   166163 Feb 15 18:06 Spark_SQL&DF-StudentCopy.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/pavanw/SparkSQL_DF/TCS_NSE.csv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put /home/pavanw/B52/SparkSQL_DF/TCS_NSE.csv /user/pavanw/SparkSQL_DF/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwxr-xr-x   - pavanw pavanw          0 2019-02-15 17:52 /user/pavanw/SparkSQL_DF/TCS_JSON\r\n",
      "-rw-r--r--   3 pavanw pavanw     265945 2019-02-15 17:50 /user/pavanw/SparkSQL_DF/TCS_NSE.csv\r\n",
      "-rw-r--r--   3 pavanw pavanw     140043 2019-02-14 15:06 /user/pavanw/SparkSQL_DF/temp_data.txt\r\n",
      "-rw-r--r--   3 pavanw pavanw     937409 2019-02-14 15:39 /user/pavanw/SparkSQL_DF/test_sample10.csv\r\n",
      "-rw-r--r--   3 pavanw pavanw   26062111 2019-02-14 16:50 /user/pavanw/SparkSQL_DF/train_sample10.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/pavanw/SparkSQL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_CSV_DF = spark.read.format(\"csv\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".load(\"/user/pavanw/SparkSQL_DF/TCS_NSE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+---------+--------+\n",
      "|               Date|      Open|      High|       Low|     Close|Adj Close|  Volume|\n",
      "+-------------------+----------+----------+----------+----------+---------+--------+\n",
      "|2004-08-25 00:00:00|149.837006|149.837006|122.375000|123.494003|67.378128|  136928|\n",
      "|2004-08-26 00:00:00|124.000000|124.625000|121.912003|122.375000|66.767601|40443200|\n",
      "|2004-08-27 00:00:00|122.800003|122.800003|119.820000|120.332001|65.652962|30646000|\n",
      "|2004-08-30 00:00:00|121.237999|123.750000|120.625000|123.345001|67.296806|24465208|\n",
      "+-------------------+----------+----------+----------+----------+---------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcs_CSV_DF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcs_CSV_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "\n",
    "tcs_CSV_DF = tcs_CSV_DF.select(col(\"Date\").cast(\"date\"), \n",
    "                     col(\"Open\").cast(\"double\"),\n",
    "                     col(\"High\").cast(\"double\"),\n",
    "                     col(\"Low\").cast(\"double\"),\n",
    "                     col(\"Close\").cast(\"double\"),\n",
    "                     col(\"Adj Close\").cast(\"double\"), \n",
    "                     col(\"Volume\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcs_CSV_DF.printSchema()countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_CSV_DF = tcs_CSV_DF.withColumnRenamed(\"Adj Close\", \"Adj_Close\")\n",
    "tcs_CSV_DF = tcs_CSV_DF.withColumnRenamed(\"Date\", \"Stock_Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_CSV_DF.write.format(\"json\").mode(\"overwrite\").save(\"/user/pavanw/SparkSQL_DF/TCS_JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON Files\n",
    "Those coming from the world of JavaScript are likely familiar with JavaScript Object Notation, or JSON, as it’s commonly called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs_JSON_DF = spark.read.format(\"json\")\\\n",
    ".option(\"inferSchema\", \"True\")\\\n",
    ".load(\"/user/pavanw/SparkSQL_DF/TCS_JSON/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+--------+\n",
      "|Adj_Close|     Close|      High|       Low|      Open|Stock_Date|  Volume|\n",
      "+---------+----------+----------+----------+----------+----------+--------+\n",
      "|67.378128|123.494003|149.837006|   122.375|149.837006|2004-08-25|  136928|\n",
      "|66.767601|   122.375|   124.625|121.912003|     124.0|2004-08-26|40443200|\n",
      "|65.652962|120.332001|122.800003|    119.82|122.800003|2004-08-27|30646000|\n",
      "|67.296806|123.345001|    123.75|   120.625|121.237999|2004-08-30|24465208|\n",
      "+---------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcs_JSON_DF.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
